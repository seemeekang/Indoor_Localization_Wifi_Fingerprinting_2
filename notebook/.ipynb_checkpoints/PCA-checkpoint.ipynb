{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class UJIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train = True, transform = None, target_transform = None, download = False):\n",
    "        self.root = root\n",
    "        dir_path = self.root + '/UJIndoorLoc'\n",
    "        zip_path = self.root + '/uji_uil.zip'\n",
    "        dataset_training_file = dir_path + '/trainingData.csv'\n",
    "        dataset_validation_file = dir_path + '/validationData.csv'\n",
    "        # Load independent variables (WAPs values)\n",
    "        if train:\n",
    "            dataset_file = dataset_training_file\n",
    "        else:\n",
    "            dataset_file = dataset_validation_file\n",
    "        file = open(dataset_file, 'r')\n",
    "        # Load independent variables\n",
    "        file_load = np.loadtxt(file, delimiter = ',', skiprows = 1)\n",
    "        self.x = file_load[:,0 : 520]\n",
    "        # Load dependent variables\n",
    "        self.y = file_load[:, 520:524]\n",
    "        file.close()\n",
    "        # Regularization of independent variables\n",
    "        self.x[self.x == 100] = -104    # WAP not detected\n",
    "        self.x = self.x + 104           # Convert into positive values\n",
    "        self.x = self.x / 104           # Regularize into scale between 0 and 1\n",
    "        # Reduce the number of dependent variables by combining building number and floor into one variable: area\n",
    "        self.y[:, 2] = self.y[:, 3] * 5 + self.y[:, 2]\n",
    "    def to_tensor(self):\n",
    "        self.x = torch.from_numpy(self.x).float()\n",
    "        self.y = torch.from_numpy(self.y).float()\n",
    "    # Return the target instance (row)\n",
    "    def __getitem__(self, index_row):\n",
    "        return self.x[index_row, :], self.y[index_row, :]\n",
    "    # Return the number of instances (the number of rows)\n",
    "    def __len__(self, dim = 0):\n",
    "        return int(self.x.size()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training dataset (`trainingData.csv`) and the test dataset (`validationData.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "dataset_train = UJIDataset('./data', train = True)\n",
    "dataset_train.to_tensor()\n",
    "# Use only a part of trainingData.csv as a training set and the leftover as a validating set\n",
    "training_size = int(0.8 * len(dataset_train))\n",
    "indices = list(range(len(dataset_train)))\n",
    "train_indices, validate_indices = indices[:training_size], indices[training_size:]\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "validate_sampler = torch.utils.data.sampler.SubsetRandomSampler(validate_indices)\n",
    "\n",
    "# Load test dataset\n",
    "dataset_test = UJIDataset('./data', train = False)\n",
    "dataset_test.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `DelegateElectArea` is based on PCA.\n",
    "It will be used to predict the building and the floor with the given input of RSSI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DelegateElectArea(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train(self, x, y, delegate_num = 1):\n",
    "        self.x_train = x\n",
    "        self.y_train = y\n",
    "        self.delegate_num = delegate_num\n",
    "    # In this case, x_dim = 520 and y_range is within 0 and 15 (area)\n",
    "    def naive_delegate(self, x_dim, y_range):\n",
    "        # Construct weight_map of size y_range by x_dim\n",
    "        self.weight_map = torch.zeros([y_range, x_dim], dtype = torch.float64)\n",
    "        y_range_count = torch.ones([y_range])\n",
    "        if self.delegate_num is 1:\n",
    "            self.x_train_delegate_index = torch.argmax(self.x_train, dim = 1)\n",
    "            # Update weight_map\n",
    "            # Weight is 1\n",
    "            for i in range(0, self.x_train_delegate_index.size()[0]):\n",
    "                self.weight_map[int(self.y_train[i]), int(self.x_train_delegate_index[i])] += 1\n",
    "                y_range_count[int(self.y_train[i])] += 1\n",
    "            self.weight_map = self.weight_map / y_range_count[:, None]\n",
    "        else:\n",
    "            # Take all features that have non-zero values as delegates with corresponding weight (based on value)\n",
    "            # self.x_train_delegate = self.x_train, no need to make a duplicate\n",
    "            # Update weight_map\n",
    "            # Weight is the RSSI value\n",
    "            for i in range(0, self.x_train.size()[0]):\n",
    "                self.weight_map[int(self.y_train[i]), :] += self.x_train[i, :]\n",
    "                y_range_count[int(self.y_train[i])] += 1\n",
    "            self.weight_map = self.weight_map / y_range_count[:, None]\n",
    "    def softmax_delegate(self, x_dim, y_range):\n",
    "        # Construct weight_map of size y_range by x_dim\n",
    "        self.weight_map = torch.zeros([y_range, x_dim], dtype = torch.float64)\n",
    "        y_range_count = torch.ones([y_range])\n",
    "        softmax_row = torch.nn.Softmax(dim = 1)\n",
    "        softmax_log = torch.nn.LogSoftmax(dim = 1)\n",
    "        # Take all features that have non-zero values as delegates with corresponding weight (based on value)\n",
    "        # self.x_train_delegate = self.x_train, no need to make a duplicate\n",
    "        # Update weight_map\n",
    "        # Applying softmax function on RSSI values for each instance\n",
    "        for i in range(0, self.x_train.size()[0]):\n",
    "            self.weight_map[int(self.y_train[i]), :] += (self.x_train[i, :])\n",
    "            y_range_count[int(self.y_train[i])] += 1\n",
    "        self.weight_map = self.weight_map / y_range_count[:, None]\n",
    "        self.weight_map = softmax_row(self.weight_map)\n",
    "    def elect(self, x_input_instance):\n",
    "        # Calculate matrix multiplication of weight_map with x_input\n",
    "        candidate = torch.matmul(self.weight_map.float(), (x_input_instance * 104).float())\n",
    "        # Return the largest weight element's index from the candidate vector\n",
    "        return torch.argmax(candidate).item()\n",
    "    def evaluate(self, x_validate, y_validate):\n",
    "        error_building = []\n",
    "        error_floor = []\n",
    "        for i in range(0, x_validate.size()[0]):\n",
    "            prediction = self.elect(x_validate[i, :])\n",
    "            if prediction != y_validate[i, 2]:\n",
    "                error_floor.append(i)\n",
    "            if int(prediction / 5) != int(y_validate[i, 2] / 5):\n",
    "                error_building.append(i)\n",
    "        error_rate_building = len(error_building) / x_validate.size()[0]\n",
    "        error_rate_floor = len(error_floor) / x_validate.size()[0]\n",
    "        print('error_rate_building: ', error_rate_building)\n",
    "        print('error_rate_floor: ', error_rate_floor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_rate_building:  0.0025075225677031092\n",
      "error_rate_floor:  0.18806419257773319\n"
     ]
    }
   ],
   "source": [
    "area_predictor = DelegateElectArea()\n",
    "area_predictor.train(dataset_train.x[0 : training_size, :], dataset_train.y[0 : training_size, 2], delegate_num = 0)\n",
    "area_predictor.softmax_delegate(x_dim = int(dataset_train.x.size()[1]), y_range = 15)\n",
    "# Cross validation\n",
    "area_predictor.evaluate(dataset_train.x[training_size : , :], dataset_train.y[training_size : ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `euclidean_distance` takes two coordinates and returns the distance between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance (unit: meter) between two coordinates in EPSG:3857 \n",
    "def euclidean_distance(latitude_1, longitude_1, latitude_2, longitude_2):\n",
    "    return np.sqrt((latitude_1 - latitude_2)**2 + (longitude_1 - longitude_2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `DelegateElectLocation` is based on PCA.\n",
    "It will be used to predict the location (latitude and longitude coordinates) with the given input of RSSI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DelegateElectLocation():\n",
    "    def __init(self):\n",
    "        pass\n",
    "    def train(self, x, y, delegate_num = 1):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.delegate_num = delegate_num\n",
    "    def naive_delegate(self):\n",
    "        # Take the leftmost, rightmost, lowermost and uppermost grid coordinates from the training data\n",
    "        self.left_bound = self.y[torch.argmin(self.y[:, 0]).item(), 0].item()\n",
    "        self.right_bound = self.y[torch.argmax(self.y[:, 0]).item(), 0].item()\n",
    "        self.lower_bound = self.y[torch.argmin(self.y[:, 1]).item(), 1].item()\n",
    "        self.upper_bound = self.y[torch.argmax(self.y[:, 1]).item(), 1].item()\n",
    "        # Scale the coordinates to natural number of which the unit becomes (0.1m)\n",
    "        column_num = int((right_bound - left_bound) * 10) + 1\n",
    "        row_num = int((upper_bound - lower_bound) * 10) + 1\n",
    "        # Construct the coordinates mapping matrix\n",
    "        self.coordinate_map = torch.zeros([row_num, column_num], dtype = torch.float64)\n",
    "        # Construct the correlation matrix of (longitude by RSSI values) and (latitude by RSSI values)\n",
    "        self.longitude_weight_map = torch.zeros([column_num, self.x.size()[1]], dtype = torch.float64)\n",
    "        self.latitude_weight_map = torch.zeros([row_num, self.x.size()[1]], dtype = torch.float64)\n",
    "        longitude_count = torch.ones([column_num])\n",
    "        latitude_count = torch.ones([row_num])\n",
    "        # If we take only one representative (largest) element of each independent instance\n",
    "        if self.delegate_num is 1:\n",
    "            delegate_index = torch.argmax(self.x, dim = 1)\n",
    "            for i in range(delegate_index.size()[0]):\n",
    "                self.longitude_weight_map[int((self.y[i, 0] - self.left_bound) * 10), int(delegate_index[i])] += 1\n",
    "                self.latitude_weight_map[int((self.y[i, 1] - self.lower_bound) * 10), int(delegate_index[i])] += 1\n",
    "                longitude_count[int((self.y[i, 0] - self.left_bound) * 10)] += 1\n",
    "                latitude_count[int((self.y[i, 1] - self.lower_bound) * 10)] += 1\n",
    "                self.coordinate_map[int((self.y[i, 1] - self.lower_bound) * 10), int((self.y[i, 0] - self.left_bound) * 10)] += 1\n",
    "        # If we take all elements of each independent instance with their corresponding weights\n",
    "        else:\n",
    "            for i in range(self.x.size()[0]):\n",
    "                self.longitude_weight_map[int((self.y[i, 0] - self.left_bound) * 10), :] += self.x[i, :]\n",
    "                self.latitude_weight_map[int((self.y[i, 1] - self.lower_bound) * 10), :] += self.x[i, :]\n",
    "                longitude_count[int((self.y[i, 0] - self.left_bound) * 10)] += 1\n",
    "                latitude_count[int((self.y[i, 1] - self.lower_bound) * 10)] += 1\n",
    "                self.coordinate_map[int((self.y[i, 1] - self.lower_bound) * 10), int((self.y[i, 0] - self.left_bound) * 10)] += 1\n",
    "        # Take means of each row of weight maps with corresponding counts\n",
    "        self.longitude_weight_map /= longitude_count[:, None]\n",
    "        self.latitude_weight_map /= latitude_count[:, None]\n",
    "    # Same as naive_delegate function except that softmax_delegate function applies softmax function to the output weight maps\n",
    "    def softmax_delegate(self):\n",
    "        self.left_bound = self.y[torch.argmin(self.y[:, 0]).item(), 0].item()\n",
    "        self.right_bound = self.y[torch.argmax(self.y[:, 0]).item(), 0].item()\n",
    "        self.lower_bound = self.y[torch.argmin(self.y[:, 1]).item(), 1].item()\n",
    "        self.upper_bound = self.y[torch.argmax(self.y[:, 1]).item(), 1].item()\n",
    "        column_num = int((self.right_bound - self.left_bound) * 10) + 1\n",
    "        row_num = int((self.upper_bound - self.lower_bound) * 10) + 1\n",
    "        self.coordinate_map = torch.zeros([row_num, column_num], dtype = torch.float64)\n",
    "        self.longitude_weight_map = torch.zeros([column_num, self.x.size()[1]], dtype = torch.float64)\n",
    "        self.latitude_weight_map = torch.zeros([row_num, self.x.size()[1]], dtype = torch.float64)\n",
    "        longitude_count = torch.ones([column_num])\n",
    "        latitude_count = torch.ones([row_num])\n",
    "        softmax_row = torch.nn.Softmax(dim = 1)\n",
    "        softmax_log = torch.nn.LogSoftmax(dim = 1)\n",
    "        # Take all features that have non-zero values as delegates with corresponding weight (based on value)\n",
    "        # self.x_train_delegate = self.x_train, no need to make a duplicate\n",
    "        # Update weight_map\n",
    "        for i in range(self.x.size()[0]):\n",
    "            self.longitude_weight_map[int((self.y[i, 0] - self.left_bound) * 10), :] += self.x[i, :]\n",
    "            self.latitude_weight_map[int((self.y[i, 1] - self.lower_bound) * 10), :] += self.x[i, :]\n",
    "            longitude_count[int((self.y[i, 0] - self.left_bound) * 10)] += 1\n",
    "            latitude_count[int((self.y[i, 1] - self.lower_bound) * 10)] += 1\n",
    "            self.coordinate_map[int((self.y[i, 1] - self.lower_bound) * 10), int((self.y[i, 0] - self.left_bound) * 10)] += 1\n",
    "        self.longitude_weight_map /= longitude_count[:, None]\n",
    "        self.latitude_weight_map /= latitude_count[:, None]\n",
    "        # Applying softmax function on RSSI values for each instance\n",
    "        self.longitude_weight_map = softmax_row(self.longitude_weight_map)\n",
    "        self.latitude_weight_map = softmax_row(self.latitude_weight_map)\n",
    "    def elect(self, x_input_instance):\n",
    "        # Calculate matrix multiplication of weight_map with transpose of x_input\n",
    "        longitude_candidate = torch.matmul(self.longitude_weight_map.float(), (x_input_instance * 104).float())\n",
    "        latitude_candidate = torch.matmul(self.latitude_weight_map.float(), (x_input_instance * 104).float())\n",
    "        # Return the largest weight element's index from the candidate vector\n",
    "        return torch.argmax(latitude_candidate).item(), torch.argmax(longitude_candidate).item()\n",
    "    def evaluate(self, x_validate, y_validate):\n",
    "        error_distance = []\n",
    "        for i in range(0, x_validate.size()[0]):\n",
    "            latitude_prediction, longitude_prediction = self.elect(x_validate[i, :])\n",
    "            # Rescale back to the original pseudo-mercator coordinates\n",
    "            latitude_prediction = latitude_prediction / 10 + self.lower_bound\n",
    "            longitude_prediction = longitude_prediction / 10 + self.left_bound\n",
    "            # Push back the errors based on euclidean distance (unit: meter) for each prediction\n",
    "            error_distance.append(euclidean_distance(y_validate[i, 1], y_validate[i, 0], latitude_prediction, longitude_prediction))\n",
    "        # Take mean, max, min of all errors of distance\n",
    "        error_mean_distance = torch.mean(torch.stack(error_distance)).item()\n",
    "        error_max_distance = torch.max(torch.stack(error_distance)).item()\n",
    "        error_min_distance = torch.min(torch.stack(error_distance)).item()\n",
    "        error_std_distance = torch.std(torch.stack(error_distance)).item()\n",
    "        error_var_distance = torch.var(torch.stack(error_distance)).item()\n",
    "        print('error_mean_distance: ', error_mean_distance)\n",
    "        print('error_max_distance: ', error_max_distance)\n",
    "        print('error_min_distance: ', error_min_distance)\n",
    "        print('error_std_distance: ', error_std_distance)\n",
    "        print('error_var_distance: ', error_var_distance)\n",
    "        #print('error_distance: ', error_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_mean_distance:  15.561348915100098\n",
      "error_max_distance:  252.39747619628906\n",
      "error_min_distance:  0.00634765625\n",
      "error_std_distance:  16.111896514892578\n",
      "error_var_distance:  259.5932312011719\n"
     ]
    }
   ],
   "source": [
    "location_predictor = DelegateElectLocation()\n",
    "location_predictor.train(dataset_train.x[0:training_size, :], dataset_train.y[0:training_size, :], delegate_num = 0)\n",
    "location_predictor.softmax_delegate()\n",
    "# Cross validation\n",
    "location_predictor.evaluate(dataset_train.x[training_size :, :], dataset_train.y[training_size :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real test with `validationData.csv` is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_rate_building:  0.0009000900090009\n",
      "error_rate_floor:  0.15031503150315031\n"
     ]
    }
   ],
   "source": [
    "area_predictor = DelegateElectArea()\n",
    "area_predictor.train(dataset_train.x, dataset_train.y[:, 2], delegate_num = 0)\n",
    "area_predictor.softmax_delegate(x_dim = int(dataset_train.x.size()[1]), y_range = 15)\n",
    "# Test (real validation)\n",
    "area_predictor.evaluate(dataset_test.x, dataset_test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_mean_distance:  13.55756664276123\n",
      "error_max_distance:  92.46202087402344\n",
      "error_min_distance:  0.0302734375\n",
      "error_std_distance:  10.330753326416016\n",
      "error_var_distance:  106.72445678710938\n"
     ]
    }
   ],
   "source": [
    "location_predictor = DelegateElectLocation()\n",
    "location_predictor.train(dataset_train.x, dataset_train.y, delegate_num = 0)\n",
    "location_predictor.softmax_delegate()\n",
    "# Test (real validation)\n",
    "location_predictor.evaluate(dataset_test.x, dataset_test.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following networks are for semi-supervised AAE (Adversarial Auto-Encoder), which needs more implementations.\n",
    "It does not work at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoder (Generator)\n",
    "class NetQ(torch.nn.Module):\n",
    "    def __init__(self, p = 0.0):\n",
    "        super(NetQ, self).__init__()\n",
    "        # global variables: x_dim = 520, y_dim = 2, h = 1000, z_dim = 5 \n",
    "        self.lin_1_x = torch.nn.Linear(x_dim, h)\n",
    "        self.lin_2_x = torch.nn.Linear(h, h)\n",
    "        self.lin_3_gaussian = torch.nn.Linear(h, z_dim)\n",
    "        self.lin_1_y = torch.nn.Linear(y_dim, h)\n",
    "        self.lin_2_y = torch.nn.Linear(h, h)\n",
    "        self.lin_3_regressive = torch.nn.Linear(h, y_dim)\n",
    "        self.p = p\n",
    "    # For Semi-supervised learning, y is added as an optional parameter\n",
    "    def forward(self, x, y = None):\n",
    "        #x = torch.nn.functional.dropout(self.lin_1_x(x), p = self.p, training = self.training)\n",
    "        x = self.lin_1_x(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        #x = torch.nn.functional.dropout(self.lin_2_x(x), p = self.p, training = self.training)\n",
    "        x = self.lin_2_x(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        z = self.lin_3_gaussian(x)\n",
    "        if y is not None:\n",
    "            #y = torch.nn.functional.dropout(self.lin_1_y(y), p = self.p, training = self.training)\n",
    "            y = self.lin_1_y(y)\n",
    "            y = torch.nn.functional.relu(y)\n",
    "            #y = torch.nn.functional.dropout(self.lin_2_y(y), p = self.p, training = self.training)\n",
    "            y = self.lin_2_y(y)\n",
    "            y = torch.nn.functional.relu(y)\n",
    "            y_fake = self.lin_3_regressive(y)\n",
    "            return z, y_fake\n",
    "        else:\n",
    "            return z\n",
    "    \n",
    "# Decoder \n",
    "class NetP(torch.nn.Module):\n",
    "    def __init__(self, p = 0.0):\n",
    "        super(NetP, self).__init__()\n",
    "        # global variables: x_dim = 520, y_dim = 2, h = 1000, z_dim = 5 \n",
    "        self.lin_1 = torch.nn.Linear(z_dim, h)\n",
    "        self.lin_2 = torch.nn.Linear(h, h)        \n",
    "        self.lin_3 = torch.nn.Linear(h, x_dim)\n",
    "        self.p = p\n",
    "    # For Semi-supervised learning, y is added as an optional parameter\n",
    "    def forward(self, x, y = None):\n",
    "        if y is not None:\n",
    "            x = torch.cat([x, y], 1)\n",
    "        #x = torch.nn.functional.dropout(self.lin_1(x), p = self.p, training = self.training)\n",
    "        x = self.lin_1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        #x = torch.nn.functional.dropout(self.lin_2(x), p = self.p, training = self.training)\n",
    "        x = self.lin_2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return self.lin_3(x)\n",
    "    \n",
    "# Gaussian Discriminator\n",
    "class NetDGaussian(torch.nn.Module):\n",
    "    def __init__(self, p = 0.0):\n",
    "        super(NetDGaussian, self).__init__()\n",
    "        # global variables: h = 1000, z_dim = 5 \n",
    "        self.lin_1 = torch.nn.Linear(z_dim, h)\n",
    "        self.lin_2 = torch.nn.Linear(h, h)\n",
    "        self.lin_3 = torch.nn.Linear(h, 1)\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        #x = torch.nn.functional.dropout(self.lin_1(x), p = self.p, training = self.training)\n",
    "        x = self.lin_1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        #x = torch.nn.functional.dropout(self.lin_2(x), p = self.p, training = self.training)\n",
    "        x = self.lin_2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return torch.softmax(self.lin_3(x), dim = 1)\n",
    "        \n",
    "# Regressive Discriminator\n",
    "class NetDRegressive(torch.nn.Module):\n",
    "    # parameter d represents distance criterion\n",
    "    def __init__(self, p = 0.0, d = 0.0):\n",
    "        super(NetDRegressive, self).__init__()\n",
    "        # global variables: y_dim = 2, h = 1000, z_dim = 5 \n",
    "        self.lin_1 = torch.nn.Linear(y_dim, h)\n",
    "        self.lin_2 = torch.nn.Linear(h, h)\n",
    "        self.lin_3 = torch.nn.Linear(h, 1)\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        #x = torch.nn.functional.dropout(self.lin_1(x), p = self.p, training = self.training)\n",
    "        x = self.lin_1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        #x = torch.nn.functional.dropout(self.lin_2(x), p = self.p, training = self.training)\n",
    "        x = self.lin_2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        return torch.softmax(self.lin_3(x), dim = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dimensions and parameters\n",
    "x_dim = dataset_train.x.size()[1]    # 520\n",
    "y_dim = 2                      # longitude and latitude\n",
    "z_dim = 5                      # latent variable dimension\n",
    "h = 1000                       # hidden layer dimension\n",
    "batch_size = 200\n",
    "n_epochs = 100\n",
    "\n",
    "torch.manual_seed(10)\n",
    "# Construct network layers\n",
    "Q = NetQ(p = 0.0)                  # Encoder (Generator)\n",
    "P = NetP(p = 0.0)                  # Decoder\n",
    "DG = NetDGaussian(p = 0.0)          # Discriminator adversarial\n",
    "DR = NetDRegressive(p = 0.0, d = 1) # Discriminator regressive\n",
    "# Activate CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    P = P.cuda()\n",
    "    Q = Q.cuda()\n",
    "    DG = DG.cuda()\n",
    "    DR = DR.cuda()\n",
    "# Set learning rate\n",
    "gen_lr, reg_lr = 0.0006, 0.0008\n",
    "# Set optimizers\n",
    "Q_encoder = torch.optim.Adam(Q.parameters(), lr = gen_lr)\n",
    "Q_generator = torch.optim.Adam(Q.parameters(), lr = reg_lr)\n",
    "P_decoder = torch.optim.Adam(P.parameters(), lr = gen_lr)\n",
    "DG_solver = torch.optim.Adam(DG.parameters(), lr = reg_lr)\n",
    "DR_solver = torch.optim.Adam(DR.parameters(), lr = reg_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction phase\n",
    "z_sample, y_sample = Q(dataset.x[0:training_size, :], dataset.y[0:training_size, 0:2])\n",
    "x_sample = P(z_sample)\n",
    "reconstruct_loss = torch.nn.functional.binary_cross_entropy(x_sample, dataset.x[0:training_size, :])\n",
    "reconstruct_loss.backward()\n",
    "P_decoder.step()\n",
    "Q_encoder.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrimination phase\n",
    "Q.eval()\n",
    "z_real = torch.autograd.Variable(torch.randn(batch_size, z_dim))\n",
    "y_real = dataset_train.y[0:training_size, 0:2]\n",
    "if torch.cuda.is_available():\n",
    "    z_real = z_real.cuda()\n",
    "    y_real = y_real.cuda()\n",
    "z_fake, y_fake = Q(dataset_train.x[0:training_size, :], dataset.y[0:training_size, 0:2])\n",
    "# Compute gaussian discriminator outputs and loss\n",
    "DG_real = DG(z_real)\n",
    "DG_fake = DG(z_fake)\n",
    "DG_loss = -torch.mean(torch.log(DG_real) + torch.log(1 - DG_fake))\n",
    "DG_loss.backward()\n",
    "DG_solver.step()\n",
    "# Compute regressive discriminator outputs and loss\n",
    "DR_real = DR(y_real)\n",
    "DR_fake = DR(y_fake)\n",
    "DR_loss = -torch.mean(torch.log(DR_real) + torch.log(1 - DR_fake))\n",
    "DR_loss.backward()\n",
    "DR_solver.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation phase\n",
    "Q.train()\n",
    "z_fake, y_fake = Q(dataset_train.x[0:training_size, :], dataset_train.y[0:training_size, 0:2])\n",
    "DG_fake = DG(z_fake)\n",
    "DR_fake = DR(y_fake)\n",
    "# Compute encoder (generator) loss\n",
    "QG_loss = -torch.mean(torch.log(DG_fake))\n",
    "QR_loss = -torch.mean(torch.log(DR_fake))\n",
    "Q_loss = QG_loss + QR_loss\n",
    "QG_loss.backward()\n",
    "R_loss.backward()\n",
    "Q_generator.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi-supervised Regression phase\n",
    "z_out, y_out = Q(dataset_train.x[0:training_size, :], dataset_train.y[0:training_size, 0:2])\n",
    "# Supervised encoder (generator) loss\n",
    "Q_supervised_loss = -torch.mean(torch.log(y_out))\n",
    "Q_encoder.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 200\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset_train, batch_size = batch_size, sampler = train_sampler)\n",
    "validate_data_loader = torch.utils.data.DataLoader(dataset_train, batch_size = batch_size, sampler = validate_sampler)\n",
    "example_x_train = next(iter(train_data_loader))\n",
    "# 50 iterations(epochs), 200 instances(batch_size) per iteration\n",
    "# 70% of the training data set will be used for training and the others for the validation\n",
    "#writer = torch.utils.tensorboard.SummaryWriter()\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch [\", epoch, \"]\", \" / [\", epochs, \"]\")\n",
    "    for i, (x, y) in enumerate(train_data_loader):\n",
    "        print(\"i: \", i)\n",
    "        #print(\"i: \", i, \"y: \", y)\n",
    "#for epoch in range(epochs):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
