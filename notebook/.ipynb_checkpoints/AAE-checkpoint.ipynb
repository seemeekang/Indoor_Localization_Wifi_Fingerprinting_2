{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "\n",
    "class UJIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train = True, transform = None, target_transform = None, download = False):\n",
    "        self.root = root\n",
    "        dir_path = self.root + '/UJIndoorLoc'\n",
    "        zip_path = self.root + '/uji_uil.zip'\n",
    "        dataset_training_file = dir_path + '/trainingData.csv'\n",
    "        dataset_validation_file = dir_path + '/validationData.csv'\n",
    "        # Load independent variables (WAPs values)\n",
    "        if train:\n",
    "            dataset_file = dataset_training_file\n",
    "        else:\n",
    "            dataset_file = dataset_validation_file\n",
    "        file = open(dataset_file, 'r')\n",
    "        # Load labels\n",
    "        label = file.readline()\n",
    "        label = label.split(',')\n",
    "        # Load independent variables\n",
    "        file_load = np.loadtxt(file, delimiter = ',', skiprows = 1)\n",
    "        #file_load_label = np.loadtxt(file, delimiter = ',')\n",
    "        #data = np.genfromtxt(file, dtype = float, delimiter = ',', names = True)\n",
    "        # RSSI values\n",
    "        self.x = file_load[:, 0 : 520]\n",
    "        # Load dependent variables\n",
    "        self.y = file_load[:, 520 : 524]\n",
    "        # Divide labels into x and y\n",
    "        self.x_label = label[0 : 520]\n",
    "        self.x_label = np.concatenate([self.x_label, label[524: 529]])\n",
    "        self.y_label = label[520 : 524]\n",
    "        # Regularization of independent variables\n",
    "        self.x[self.x == 100] = np.nan    # WAP not detected\n",
    "        self.x = self.x + 104             # Convert into positive values\n",
    "        self.x = self.x / 104             # Regularize into scale between 0 and 1\n",
    "        # Building ID, Space ID, Relative Position, User ID, Phone ID and Timestamp respectively\n",
    "        self.x = np.concatenate([self.x, file_load[:, 524 : 529]], axis = 1)\n",
    "        file.close()\n",
    "        # Reduce the number of dependent variables by combining building number and floor into one variable: area\n",
    "        area = self.y[:, 3] * 5 + self.y[:, 2]\n",
    "        self.y = np.column_stack((self.y, area))\n",
    "    def to_tensor(self):\n",
    "        self.x = torch.from_numpy(self.x).float()\n",
    "        self.y = torch.from_numpy(self.y).float()\n",
    "        self.area = torch.from_numpy(self.area).float()\n",
    "    def nan_to_zero(self):\n",
    "        self.x = np.nan_to_num(self.x)\n",
    "    # Return the target instance (row)\n",
    "    def __getitem__(self, index_row):\n",
    "        return self.x[index_row, :], self.y[index_row, :]\n",
    "    # Return the number of instances (the number of rows)\n",
    "    def __len__(self, dim = 0):\n",
    "        return int(self.x.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance (unit: meter) between two coordinates in EPSG:3857 \n",
    "def euclidean_distance(latitude_1, longitude_1, latitude_2, longitude_2):\n",
    "    return np.sqrt((latitude_1 - latitude_2)**2 + (longitude_1 - longitude_2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset with lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0., 0., 1., 1.],\n",
      "        [0., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros([2,2])\n",
    "print(a)\n",
    "b = torch.ones([2,2])\n",
    "c = torch.cat((a, b), dim = 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "dataset_train = UJIDataset('./data', train = True)\n",
    "dataset_test = UJIDataset('./data', train = False)\n",
    "#gb_train_data = lgb.Dataset("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train.nan_to_zero()\n",
    "#dataset_validate.nan_to_zero()\n",
    "#dataset_test.nan_to_zero()\n",
    "#dataset_train.to_tensor()\n",
    "#dataset_validate.to_tensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37171369e+09]\n",
      " [           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37171410e+09]\n",
      " [           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37171381e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 1.80000000e+01\n",
      "  1.00000000e+01 1.37171092e+09]\n",
      " [           nan            nan            nan ... 1.80000000e+01\n",
      "  1.00000000e+01 1.37171105e+09]\n",
      " [           nan            nan            nan ... 1.80000000e+01\n",
      "  1.00000000e+01 1.37171102e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115505e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115510e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115514e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124778e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124781e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124784e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = sklearn.model_selection.train_test_split(dataset_train.x, dataset_train.y, test_size = 0.2, random_state = 42)\n",
    "x_test1, x_test2, y_test1, y_test2 = sklearn.model_selection.train_test_split(dataset_test.x, dataset_test.y, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 1.00000000e+00\n",
      "  1.40000000e+01 1.37087874e+09]\n",
      " [           nan            nan            nan ... 6.00000000e+00\n",
      "  1.90000000e+01 1.37171987e+09]\n",
      " [           nan            nan            nan ... 1.00000000e+01\n",
      "  8.00000000e+00 1.37172087e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 9.00000000e+00\n",
      "  1.40000000e+01 1.37172008e+09]\n",
      " [           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37172121e+09]\n",
      " [           nan            nan            nan ... 1.00000000e+00\n",
      "  1.40000000e+01 1.37103670e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_reg_long_lgb = lgb.Dataset(x_train, label = y_train[:, 0])\n",
    "dataset_validate_reg_long_lgb = lgb.Dataset(x_validate, label = y_validate[:, 0])\n",
    "dataset_train_reg_lat_lgb = lgb.Dataset(x_train, label = y_train[:, 1])\n",
    "dataset_validate_reg_lat_lgb = lgb.Dataset(x_validate, label = y_validate[:, 1])\n",
    "dataset_train_cat_floor_lgb = lgb.Dataset(x_train, label = y_train[:, 2])\n",
    "dataset_validate_cat_floor_lgb = lgb.Dataset(x_validate, label = y_validate[:, 2])\n",
    "dataset_train_cat_building_lgb = lgb.Dataset(x_train, label = y_train[:, 3])\n",
    "dataset_validate_cat_building_lgb = lgb.Dataset(x_validate, label = y_validate[:, 3])\n",
    "dataset_train_cat_area_lgb = lgb.Dataset(x_train, label = y_train[:, 4])\n",
    "dataset_validate_cat_area_lgb = lgb.Dataset(x_validate, label = y_validate[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_reg = {'learning_rate': 0.001,\n",
    "              'num_boost_round': 1000,\n",
    "              'max_depth': 10,\n",
    "              'boosting': 'rf', \n",
    "              'objective': 'regression',\n",
    "              'metric': 'rmse', \n",
    "              'is_training_metric': True, \n",
    "              'num_leaves': 1440,  \n",
    "              #'feature_fraction': 0.7, \n",
    "              'bagging_fraction': 0.8, \n",
    "              'bagging_freq': 5,\n",
    "              'early_stopping_round': 100,\n",
    "              #'seed':2018\n",
    "             }\n",
    "params_cat_building = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 3,\n",
    "              'metric': 'multi_logloss',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              'early_stopping_round': 100,\n",
    "             }\n",
    "params_cat_floor = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 5,\n",
    "              'metric': 'multi_logloss',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              'early_stopping_round': 100,\n",
    "             }\n",
    "params_cat_area = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 15,\n",
    "              'metric': 'multi_logloss',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              'early_stopping_round': 100,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgbr = {\n",
    "              'boosting_type': 'rf',\n",
    "              'bagging_freq': 5,\n",
    "              'bagging_fraction': 0.8,\n",
    "              'num_leaves': 1440,  \n",
    "              'learning_rate': 0.001,\n",
    "              'max_depth': -1,\n",
    "              'objective': 'regression'\n",
    "              #'seed':2018\n",
    "              }\n",
    "\n",
    "params_lgbr_long_fit = {'eval_set': [(x_validate, y_validate[:, 0])],\n",
    "                        'eval_names': ['evalset_long'],\n",
    "                        'eval_metric': ['rmse'],\n",
    "                        #'early_stoppping_rounds': [100],\n",
    "                       }\n",
    "\n",
    "params_lgbr_lat_fit = {'eval_set': [(x_validate, y_validate[:, 1])],\n",
    "                        'eval_names': ['evalset_lat'],\n",
    "                        'eval_metric': ['rmse'],\n",
    "                        #'early_stoppping_rounds': [100],\n",
    "                       }\n",
    "\n",
    "params_lgbc = {'boosting_type': 'rf',\n",
    "               'bagging_freq': 5,\n",
    "               'bagging_fraction': 0.8,\n",
    "               'num_leaves': 144,  \n",
    "               'learning_rate': 0.002,\n",
    "               'max_depth': -1,\n",
    "               'objective': 'multiclass'\n",
    "              }\n",
    "\n",
    "params_lgbc_building_fit = {'eval_set': [(x_validate, y_validate[:, 2])],\n",
    "                            'eval_names': ['evalset_building'],\n",
    "                            'eval_metric': ['multi_logloss'],\n",
    "                            #'early_stoppping_rounds': [100],\n",
    "                           }\n",
    "\n",
    "params_lgbc_floor_fit = {'eval_set': [(x_validate, y_validate[:, 3])],\n",
    "                         'eval_names': ['evalset_floor'],\n",
    "                         'eval_metric': ['multi_logloss'],\n",
    "                         #'early_stopping_rounds': [100],\n",
    "                        }\n",
    "\n",
    "params_lgbc_area_fit = {'eval_set': [(x_validate, y_validate[:, 4])],\n",
    "                        'eval_names': ['evalset_area'],\n",
    "                        'eval_metric': ['multi_logloss'],\n",
    "                        #'early_stopping_rounds': [100],\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feval(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool\n",
    "#def multiclass_score(preds, train_data):\n",
    "#    label = train_data.get_label()\n",
    "#    return 'multiclass_score', np.mean(label == preds), True\n",
    "\n",
    "#def regression_score(preds, train_data):\n",
    "#    label = train_data.get_label()\n",
    "#    return 'regression_score', np.mean(euclidean_distance(label, 0, preds, 0)), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lees/anaconda3/envs/pytorch_3.7/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For early stopping, at least one dataset and eval metric is required for evaluation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7f65da30e076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_reg_long_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_reg_long_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_reg_long_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_reg_lat_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_reg_lat_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_reg_lat_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_cat_floor_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_cat_floor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_cat_floor_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_cat_floor_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_cat_building_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_cat_building\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_cat_building_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_cat_building_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_cat_area_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_cat_area\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_cat_area_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_cat_area_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_3.7/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    262\u001b[0m                                         \u001b[0mbegin_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                                         \u001b[0mend_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                                         evaluation_result_list=evaluation_result_list))\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_3.7/lib/python3.7/site-packages/lightgbm/callback.py\u001b[0m in \u001b[0;36m_callback\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcmp_op\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_3.7/lib/python3.7/site-packages/lightgbm/callback.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             raise ValueError('For early stopping, '\n\u001b[0m\u001b[1;32m    196\u001b[0m                              'at least one dataset and eval metric is required for evaluation')\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: For early stopping, at least one dataset and eval metric is required for evaluation"
     ]
    }
   ],
   "source": [
    "model_reg_long_lgb = lgb.train(params_reg, dataset_train_reg_long_lgb, dataset_validate_reg_long_lgb, verbose_eval = 100)# early_stopping_rounds = 100)\n",
    "model_reg_lat_lgb = lgb.train(params_reg, dataset_train_reg_lat_lgb, dataset_validate_reg_lat_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_cat_floor_lgb = lgb.train(params_cat_floor, dataset_train_cat_floor_lgb, dataset_validate_cat_floor_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_cat_building_lgb = lgb.train(params_cat_building, dataset_train_cat_building_lgb, dataset_validate_cat_building_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_cat_area_lgb = lgb.train(params_cat_area, dataset_train_cat_area_lgb, dataset_validate_cat_area_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tevalset_long's rmse: 7.74875\tevalset_long's l2: 60.0432\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tevalset_long's rmse: 7.74875\tevalset_long's l2: 60.0432\n",
      "[3]\tevalset_long's rmse: 7.74875\tevalset_long's l2: 60.0432\n",
      "[4]\tevalset_long's rmse: 7.74875\tevalset_long's l2: 60.0432\n",
      "[5]\tevalset_long's rmse: 7.74875\tevalset_long's l2: 60.0432\n",
      "[6]\tevalset_long's rmse: 7.10206\tevalset_long's l2: 50.4393\n",
      "[7]\tevalset_long's rmse: 6.78125\tevalset_long's l2: 45.9853\n",
      "[8]\tevalset_long's rmse: 6.63028\tevalset_long's l2: 43.9606\n",
      "[9]\tevalset_long's rmse: 6.57005\tevalset_long's l2: 43.1655\n",
      "[10]\tevalset_long's rmse: 6.55901\tevalset_long's l2: 43.0207\n",
      "[11]\tevalset_long's rmse: 6.41496\tevalset_long's l2: 41.1517\n",
      "[12]\tevalset_long's rmse: 6.34208\tevalset_long's l2: 40.2219\n",
      "[13]\tevalset_long's rmse: 6.31513\tevalset_long's l2: 39.8809\n",
      "[14]\tevalset_long's rmse: 6.31773\tevalset_long's l2: 39.9137\n",
      "[15]\tevalset_long's rmse: 6.33916\tevalset_long's l2: 40.185\n",
      "[16]\tevalset_long's rmse: 6.24881\tevalset_long's l2: 39.0476\n",
      "[17]\tevalset_long's rmse: 6.19268\tevalset_long's l2: 38.3492\n",
      "[18]\tevalset_long's rmse: 6.16187\tevalset_long's l2: 37.9687\n",
      "[19]\tevalset_long's rmse: 6.14979\tevalset_long's l2: 37.8199\n",
      "[20]\tevalset_long's rmse: 6.15148\tevalset_long's l2: 37.8407\n",
      "[21]\tevalset_long's rmse: 6.09751\tevalset_long's l2: 37.1797\n",
      "[22]\tevalset_long's rmse: 6.06206\tevalset_long's l2: 36.7486\n",
      "[23]\tevalset_long's rmse: 6.04121\tevalset_long's l2: 36.4963\n",
      "[24]\tevalset_long's rmse: 6.03187\tevalset_long's l2: 36.3834\n",
      "[25]\tevalset_long's rmse: 6.03155\tevalset_long's l2: 36.3796\n",
      "[26]\tevalset_long's rmse: 5.97897\tevalset_long's l2: 35.7481\n",
      "[27]\tevalset_long's rmse: 5.93938\tevalset_long's l2: 35.2763\n",
      "[28]\tevalset_long's rmse: 5.91061\tevalset_long's l2: 34.9353\n",
      "[29]\tevalset_long's rmse: 5.89084\tevalset_long's l2: 34.702\n",
      "[30]\tevalset_long's rmse: 5.87856\tevalset_long's l2: 34.5574\n",
      "[31]\tevalset_long's rmse: 5.88672\tevalset_long's l2: 34.6534\n",
      "[32]\tevalset_long's rmse: 5.90168\tevalset_long's l2: 34.8298\n",
      "[33]\tevalset_long's rmse: 5.92213\tevalset_long's l2: 35.0717\n",
      "[34]\tevalset_long's rmse: 5.94699\tevalset_long's l2: 35.3667\n",
      "[35]\tevalset_long's rmse: 5.97535\tevalset_long's l2: 35.7048\n",
      "[36]\tevalset_long's rmse: 5.96945\tevalset_long's l2: 35.6343\n",
      "[37]\tevalset_long's rmse: 5.9688\tevalset_long's l2: 35.6266\n",
      "[38]\tevalset_long's rmse: 5.9726\tevalset_long's l2: 35.672\n",
      "[39]\tevalset_long's rmse: 5.98018\tevalset_long's l2: 35.7625\n",
      "[40]\tevalset_long's rmse: 5.99094\tevalset_long's l2: 35.8914\n",
      "[41]\tevalset_long's rmse: 5.9695\tevalset_long's l2: 35.6349\n",
      "[42]\tevalset_long's rmse: 5.95304\tevalset_long's l2: 35.4387\n",
      "[43]\tevalset_long's rmse: 5.941\tevalset_long's l2: 35.2954\n",
      "[44]\tevalset_long's rmse: 5.93283\tevalset_long's l2: 35.1985\n",
      "[45]\tevalset_long's rmse: 5.92808\tevalset_long's l2: 35.1422\n",
      "[46]\tevalset_long's rmse: 5.93172\tevalset_long's l2: 35.1852\n",
      "[47]\tevalset_long's rmse: 5.93809\tevalset_long's l2: 35.2609\n",
      "[48]\tevalset_long's rmse: 5.94684\tevalset_long's l2: 35.3649\n",
      "[49]\tevalset_long's rmse: 5.95766\tevalset_long's l2: 35.4937\n",
      "[50]\tevalset_long's rmse: 5.97027\tevalset_long's l2: 35.6441\n",
      "[51]\tevalset_long's rmse: 5.96698\tevalset_long's l2: 35.6049\n",
      "[52]\tevalset_long's rmse: 5.96673\tevalset_long's l2: 35.6019\n",
      "[53]\tevalset_long's rmse: 5.96919\tevalset_long's l2: 35.6313\n",
      "[54]\tevalset_long's rmse: 5.97406\tevalset_long's l2: 35.6894\n",
      "[55]\tevalset_long's rmse: 5.98106\tevalset_long's l2: 35.7731\n",
      "[56]\tevalset_long's rmse: 5.99674\tevalset_long's l2: 35.9609\n",
      "[57]\tevalset_long's rmse: 6.01336\tevalset_long's l2: 36.1605\n",
      "[58]\tevalset_long's rmse: 6.03077\tevalset_long's l2: 36.3702\n",
      "[59]\tevalset_long's rmse: 6.04887\tevalset_long's l2: 36.5888\n",
      "[60]\tevalset_long's rmse: 6.06754\tevalset_long's l2: 36.815\n",
      "[61]\tevalset_long's rmse: 6.07698\tevalset_long's l2: 36.9297\n",
      "[62]\tevalset_long's rmse: 6.08744\tevalset_long's l2: 37.057\n",
      "[63]\tevalset_long's rmse: 6.09882\tevalset_long's l2: 37.1956\n",
      "[64]\tevalset_long's rmse: 6.111\tevalset_long's l2: 37.3443\n",
      "[65]\tevalset_long's rmse: 6.12388\tevalset_long's l2: 37.5019\n",
      "[66]\tevalset_long's rmse: 6.12286\tevalset_long's l2: 37.4894\n",
      "[67]\tevalset_long's rmse: 6.12319\tevalset_long's l2: 37.4934\n",
      "[68]\tevalset_long's rmse: 6.12475\tevalset_long's l2: 37.5125\n",
      "[69]\tevalset_long's rmse: 6.12743\tevalset_long's l2: 37.5454\n",
      "[70]\tevalset_long's rmse: 6.13113\tevalset_long's l2: 37.5908\n",
      "[71]\tevalset_long's rmse: 6.13986\tevalset_long's l2: 37.6979\n",
      "[72]\tevalset_long's rmse: 6.14929\tevalset_long's l2: 37.8137\n",
      "[73]\tevalset_long's rmse: 6.15933\tevalset_long's l2: 37.9373\n",
      "[74]\tevalset_long's rmse: 6.16993\tevalset_long's l2: 38.068\n",
      "[75]\tevalset_long's rmse: 6.18102\tevalset_long's l2: 38.205\n",
      "[76]\tevalset_long's rmse: 6.17908\tevalset_long's l2: 38.1811\n",
      "[77]\tevalset_long's rmse: 6.17845\tevalset_long's l2: 38.1733\n",
      "[78]\tevalset_long's rmse: 6.17903\tevalset_long's l2: 38.1804\n",
      "[79]\tevalset_long's rmse: 6.18072\tevalset_long's l2: 38.2013\n",
      "[80]\tevalset_long's rmse: 6.18345\tevalset_long's l2: 38.235\n",
      "[81]\tevalset_long's rmse: 6.17727\tevalset_long's l2: 38.1587\n",
      "[82]\tevalset_long's rmse: 6.17219\tevalset_long's l2: 38.0959\n",
      "[83]\tevalset_long's rmse: 6.16813\tevalset_long's l2: 38.0458\n",
      "[84]\tevalset_long's rmse: 6.16503\tevalset_long's l2: 38.0075\n",
      "[85]\tevalset_long's rmse: 6.16282\tevalset_long's l2: 37.9803\n",
      "[86]\tevalset_long's rmse: 6.1618\tevalset_long's l2: 37.9677\n",
      "[87]\tevalset_long's rmse: 6.16154\tevalset_long's l2: 37.9646\n",
      "[88]\tevalset_long's rmse: 6.16199\tevalset_long's l2: 37.9701\n",
      "[89]\tevalset_long's rmse: 6.1631\tevalset_long's l2: 37.9838\n",
      "[90]\tevalset_long's rmse: 6.16483\tevalset_long's l2: 38.0051\n",
      "[91]\tevalset_long's rmse: 6.1631\tevalset_long's l2: 37.9837\n",
      "[92]\tevalset_long's rmse: 6.16214\tevalset_long's l2: 37.9719\n",
      "[93]\tevalset_long's rmse: 6.16191\tevalset_long's l2: 37.9691\n",
      "[94]\tevalset_long's rmse: 6.16236\tevalset_long's l2: 37.9746\n",
      "[95]\tevalset_long's rmse: 6.16345\tevalset_long's l2: 37.9881\n",
      "[96]\tevalset_long's rmse: 6.17335\tevalset_long's l2: 38.1103\n",
      "[97]\tevalset_long's rmse: 6.18365\tevalset_long's l2: 38.2375\n",
      "[98]\tevalset_long's rmse: 6.19431\tevalset_long's l2: 38.3695\n",
      "[99]\tevalset_long's rmse: 6.20531\tevalset_long's l2: 38.5059\n",
      "[100]\tevalset_long's rmse: 6.21661\tevalset_long's l2: 38.6462\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[30]\tevalset_long's rmse: 5.87856\tevalset_long's l2: 34.5574\n",
      "[1]\tevalset_lat's rmse: 6.76066\tevalset_lat's l2: 45.7065\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tevalset_lat's rmse: 6.76066\tevalset_lat's l2: 45.7065\n",
      "[3]\tevalset_lat's rmse: 6.76066\tevalset_lat's l2: 45.7065\n",
      "[4]\tevalset_lat's rmse: 6.76066\tevalset_lat's l2: 45.7065\n",
      "[5]\tevalset_lat's rmse: 6.76066\tevalset_lat's l2: 45.7065\n",
      "[6]\tevalset_lat's rmse: 6.19823\tevalset_lat's l2: 38.4181\n",
      "[7]\tevalset_lat's rmse: 5.89721\tevalset_lat's l2: 34.7771\n",
      "[8]\tevalset_lat's rmse: 5.73605\tevalset_lat's l2: 32.9023\n",
      "[9]\tevalset_lat's rmse: 5.65254\tevalset_lat's l2: 31.9512\n",
      "[10]\tevalset_lat's rmse: 5.61337\tevalset_lat's l2: 31.5099\n",
      "[11]\tevalset_lat's rmse: 5.48978\tevalset_lat's l2: 30.1377\n",
      "[12]\tevalset_lat's rmse: 5.43028\tevalset_lat's l2: 29.4879\n",
      "[13]\tevalset_lat's rmse: 5.41191\tevalset_lat's l2: 29.2888\n",
      "[14]\tevalset_lat's rmse: 5.41977\tevalset_lat's l2: 29.3739\n",
      "[15]\tevalset_lat's rmse: 5.44414\tevalset_lat's l2: 29.6387\n",
      "[16]\tevalset_lat's rmse: 5.41003\tevalset_lat's l2: 29.2684\n",
      "[17]\tevalset_lat's rmse: 5.39514\tevalset_lat's l2: 29.1076\n",
      "[18]\tevalset_lat's rmse: 5.39398\tevalset_lat's l2: 29.0951\n",
      "[19]\tevalset_lat's rmse: 5.40259\tevalset_lat's l2: 29.188\n",
      "[20]\tevalset_lat's rmse: 5.41809\tevalset_lat's l2: 29.3557\n",
      "[21]\tevalset_lat's rmse: 5.39473\tevalset_lat's l2: 29.1031\n",
      "[22]\tevalset_lat's rmse: 5.38269\tevalset_lat's l2: 28.9733\n",
      "[23]\tevalset_lat's rmse: 5.37939\tevalset_lat's l2: 28.9379\n",
      "[24]\tevalset_lat's rmse: 5.38285\tevalset_lat's l2: 28.975\n",
      "[25]\tevalset_lat's rmse: 5.39148\tevalset_lat's l2: 29.0681\n",
      "[26]\tevalset_lat's rmse: 5.4057\tevalset_lat's l2: 29.2216\n",
      "[27]\tevalset_lat's rmse: 5.42353\tevalset_lat's l2: 29.4147\n",
      "[28]\tevalset_lat's rmse: 5.44408\tevalset_lat's l2: 29.638\n",
      "[29]\tevalset_lat's rmse: 5.46662\tevalset_lat's l2: 29.8839\n",
      "[30]\tevalset_lat's rmse: 5.49058\tevalset_lat's l2: 30.1465\n",
      "[31]\tevalset_lat's rmse: 5.4878\tevalset_lat's l2: 30.116\n",
      "[32]\tevalset_lat's rmse: 5.48835\tevalset_lat's l2: 30.122\n",
      "[33]\tevalset_lat's rmse: 5.49165\tevalset_lat's l2: 30.1582\n",
      "[34]\tevalset_lat's rmse: 5.4972\tevalset_lat's l2: 30.2192\n",
      "[35]\tevalset_lat's rmse: 5.50462\tevalset_lat's l2: 30.3008\n",
      "[36]\tevalset_lat's rmse: 5.49783\tevalset_lat's l2: 30.2261\n",
      "[37]\tevalset_lat's rmse: 5.49389\tevalset_lat's l2: 30.1828\n",
      "[38]\tevalset_lat's rmse: 5.49239\tevalset_lat's l2: 30.1664\n",
      "[39]\tevalset_lat's rmse: 5.49297\tevalset_lat's l2: 30.1727\n",
      "[40]\tevalset_lat's rmse: 5.49534\tevalset_lat's l2: 30.1987\n",
      "[41]\tevalset_lat's rmse: 5.46066\tevalset_lat's l2: 29.8188\n",
      "[42]\tevalset_lat's rmse: 5.43167\tevalset_lat's l2: 29.503\n",
      "[43]\tevalset_lat's rmse: 5.40773\tevalset_lat's l2: 29.2435\n",
      "[44]\tevalset_lat's rmse: 5.38831\tevalset_lat's l2: 29.0339\n",
      "[45]\tevalset_lat's rmse: 5.37292\tevalset_lat's l2: 28.8682\n",
      "[46]\tevalset_lat's rmse: 5.34787\tevalset_lat's l2: 28.5997\n",
      "[47]\tevalset_lat's rmse: 5.32607\tevalset_lat's l2: 28.3671\n",
      "[48]\tevalset_lat's rmse: 5.30722\tevalset_lat's l2: 28.1666\n",
      "[49]\tevalset_lat's rmse: 5.29103\tevalset_lat's l2: 27.995\n",
      "[50]\tevalset_lat's rmse: 5.27725\tevalset_lat's l2: 27.8493\n",
      "[51]\tevalset_lat's rmse: 5.28568\tevalset_lat's l2: 27.9384\n",
      "[52]\tevalset_lat's rmse: 5.29504\tevalset_lat's l2: 28.0374\n",
      "[53]\tevalset_lat's rmse: 5.3052\tevalset_lat's l2: 28.1452\n",
      "[54]\tevalset_lat's rmse: 5.31606\tevalset_lat's l2: 28.2605\n",
      "[55]\tevalset_lat's rmse: 5.3275\tevalset_lat's l2: 28.3823\n",
      "[56]\tevalset_lat's rmse: 5.31553\tevalset_lat's l2: 28.2549\n",
      "[57]\tevalset_lat's rmse: 5.30503\tevalset_lat's l2: 28.1433\n",
      "[58]\tevalset_lat's rmse: 5.29586\tevalset_lat's l2: 28.0462\n",
      "[59]\tevalset_lat's rmse: 5.28793\tevalset_lat's l2: 27.9622\n",
      "[60]\tevalset_lat's rmse: 5.28112\tevalset_lat's l2: 27.8902\n",
      "[61]\tevalset_lat's rmse: 5.28793\tevalset_lat's l2: 27.9622\n",
      "[62]\tevalset_lat's rmse: 5.29536\tevalset_lat's l2: 28.0408\n",
      "[63]\tevalset_lat's rmse: 5.30332\tevalset_lat's l2: 28.1252\n",
      "[64]\tevalset_lat's rmse: 5.31176\tevalset_lat's l2: 28.2148\n",
      "[65]\tevalset_lat's rmse: 5.32061\tevalset_lat's l2: 28.3089\n",
      "[66]\tevalset_lat's rmse: 5.32731\tevalset_lat's l2: 28.3802\n",
      "[67]\tevalset_lat's rmse: 5.33443\tevalset_lat's l2: 28.4561\n",
      "[68]\tevalset_lat's rmse: 5.34193\tevalset_lat's l2: 28.5362\n",
      "[69]\tevalset_lat's rmse: 5.34977\tevalset_lat's l2: 28.62\n",
      "[70]\tevalset_lat's rmse: 5.3579\tevalset_lat's l2: 28.7071\n",
      "[71]\tevalset_lat's rmse: 5.34714\tevalset_lat's l2: 28.5919\n",
      "[72]\tevalset_lat's rmse: 5.33758\tevalset_lat's l2: 28.4898\n",
      "[73]\tevalset_lat's rmse: 5.32915\tevalset_lat's l2: 28.3999\n",
      "[74]\tevalset_lat's rmse: 5.32177\tevalset_lat's l2: 28.3212\n",
      "[75]\tevalset_lat's rmse: 5.31536\tevalset_lat's l2: 28.2531\n",
      "[76]\tevalset_lat's rmse: 5.31528\tevalset_lat's l2: 28.2522\n",
      "[77]\tevalset_lat's rmse: 5.31592\tevalset_lat's l2: 28.259\n",
      "[78]\tevalset_lat's rmse: 5.31723\tevalset_lat's l2: 28.273\n",
      "[79]\tevalset_lat's rmse: 5.31917\tevalset_lat's l2: 28.2936\n",
      "[80]\tevalset_lat's rmse: 5.32168\tevalset_lat's l2: 28.3203\n",
      "[81]\tevalset_lat's rmse: 5.32684\tevalset_lat's l2: 28.3752\n",
      "[82]\tevalset_lat's rmse: 5.33227\tevalset_lat's l2: 28.4331\n",
      "[83]\tevalset_lat's rmse: 5.33793\tevalset_lat's l2: 28.4935\n",
      "[84]\tevalset_lat's rmse: 5.34381\tevalset_lat's l2: 28.5564\n",
      "[85]\tevalset_lat's rmse: 5.34989\tevalset_lat's l2: 28.6214\n",
      "[86]\tevalset_lat's rmse: 5.35424\tevalset_lat's l2: 28.6679\n",
      "[87]\tevalset_lat's rmse: 5.35884\tevalset_lat's l2: 28.7172\n",
      "[88]\tevalset_lat's rmse: 5.36369\tevalset_lat's l2: 28.7692\n",
      "[89]\tevalset_lat's rmse: 5.36875\tevalset_lat's l2: 28.8235\n",
      "[90]\tevalset_lat's rmse: 5.37402\tevalset_lat's l2: 28.8801\n",
      "[91]\tevalset_lat's rmse: 5.37727\tevalset_lat's l2: 28.9151\n",
      "[92]\tevalset_lat's rmse: 5.38092\tevalset_lat's l2: 28.9542\n",
      "[93]\tevalset_lat's rmse: 5.38491\tevalset_lat's l2: 28.9973\n",
      "[94]\tevalset_lat's rmse: 5.38923\tevalset_lat's l2: 29.0438\n",
      "[95]\tevalset_lat's rmse: 5.39386\tevalset_lat's l2: 29.0937\n",
      "[96]\tevalset_lat's rmse: 5.39646\tevalset_lat's l2: 29.1218\n",
      "[97]\tevalset_lat's rmse: 5.39944\tevalset_lat's l2: 29.154\n",
      "[98]\tevalset_lat's rmse: 5.40279\tevalset_lat's l2: 29.1901\n",
      "[99]\tevalset_lat's rmse: 5.40647\tevalset_lat's l2: 29.2299\n",
      "[100]\tevalset_lat's rmse: 5.41046\tevalset_lat's l2: 29.2731\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[50]\tevalset_lat's rmse: 5.27725\tevalset_lat's l2: 27.8493\n",
      "[1]\tevalset_floor's multi_logloss: 2.61803\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tevalset_floor's multi_logloss: 2.61803\n",
      "[3]\tevalset_floor's multi_logloss: 2.61803\n",
      "[4]\tevalset_floor's multi_logloss: 2.61803\n",
      "[5]\tevalset_floor's multi_logloss: 2.61803\n",
      "[6]\tevalset_floor's multi_logloss: 2.61214\n",
      "[7]\tevalset_floor's multi_logloss: 2.6089\n",
      "[8]\tevalset_floor's multi_logloss: 2.60702\n",
      "[9]\tevalset_floor's multi_logloss: 2.60588\n",
      "[10]\tevalset_floor's multi_logloss: 2.60518\n",
      "[11]\tevalset_floor's multi_logloss: 2.60264\n",
      "[12]\tevalset_floor's multi_logloss: 2.60082\n",
      "[13]\tevalset_floor's multi_logloss: 2.59952\n",
      "[14]\tevalset_floor's multi_logloss: 2.59858\n",
      "[15]\tevalset_floor's multi_logloss: 2.5979\n",
      "[16]\tevalset_floor's multi_logloss: 2.59728\n",
      "[17]\tevalset_floor's multi_logloss: 2.59685\n",
      "[18]\tevalset_floor's multi_logloss: 2.59655\n",
      "[19]\tevalset_floor's multi_logloss: 2.59636\n",
      "[20]\tevalset_floor's multi_logloss: 2.59625\n",
      "[21]\tevalset_floor's multi_logloss: 2.59638\n",
      "[22]\tevalset_floor's multi_logloss: 2.59656\n",
      "[23]\tevalset_floor's multi_logloss: 2.59678\n",
      "[24]\tevalset_floor's multi_logloss: 2.59703\n",
      "[25]\tevalset_floor's multi_logloss: 2.59729\n",
      "[26]\tevalset_floor's multi_logloss: 2.59674\n",
      "[27]\tevalset_floor's multi_logloss: 2.59628\n",
      "[28]\tevalset_floor's multi_logloss: 2.5959\n",
      "[29]\tevalset_floor's multi_logloss: 2.59559\n",
      "[30]\tevalset_floor's multi_logloss: 2.59535\n",
      "[31]\tevalset_floor's multi_logloss: 2.59485\n",
      "[32]\tevalset_floor's multi_logloss: 2.59441\n",
      "[33]\tevalset_floor's multi_logloss: 2.59403\n",
      "[34]\tevalset_floor's multi_logloss: 2.5937\n",
      "[35]\tevalset_floor's multi_logloss: 2.59341\n",
      "[36]\tevalset_floor's multi_logloss: 2.59331\n",
      "[37]\tevalset_floor's multi_logloss: 2.59325\n",
      "[38]\tevalset_floor's multi_logloss: 2.59321\n",
      "[39]\tevalset_floor's multi_logloss: 2.59319\n",
      "[40]\tevalset_floor's multi_logloss: 2.59319\n",
      "[41]\tevalset_floor's multi_logloss: 2.59328\n",
      "[42]\tevalset_floor's multi_logloss: 2.59338\n",
      "[43]\tevalset_floor's multi_logloss: 2.5935\n",
      "[44]\tevalset_floor's multi_logloss: 2.59362\n",
      "[45]\tevalset_floor's multi_logloss: 2.59375\n",
      "[46]\tevalset_floor's multi_logloss: 2.5936\n",
      "[47]\tevalset_floor's multi_logloss: 2.59348\n",
      "[48]\tevalset_floor's multi_logloss: 2.59337\n",
      "[49]\tevalset_floor's multi_logloss: 2.59328\n",
      "[50]\tevalset_floor's multi_logloss: 2.59321\n",
      "[51]\tevalset_floor's multi_logloss: 2.59311\n",
      "[52]\tevalset_floor's multi_logloss: 2.59303\n",
      "[53]\tevalset_floor's multi_logloss: 2.59296\n",
      "[54]\tevalset_floor's multi_logloss: 2.59291\n",
      "[55]\tevalset_floor's multi_logloss: 2.59286\n",
      "[56]\tevalset_floor's multi_logloss: 2.59273\n",
      "[57]\tevalset_floor's multi_logloss: 2.59262\n",
      "[58]\tevalset_floor's multi_logloss: 2.59252\n",
      "[59]\tevalset_floor's multi_logloss: 2.59243\n",
      "[60]\tevalset_floor's multi_logloss: 2.59235\n",
      "[61]\tevalset_floor's multi_logloss: 2.59231\n",
      "[62]\tevalset_floor's multi_logloss: 2.59227\n",
      "[63]\tevalset_floor's multi_logloss: 2.59225\n",
      "[64]\tevalset_floor's multi_logloss: 2.59223\n",
      "[65]\tevalset_floor's multi_logloss: 2.59222\n",
      "[66]\tevalset_floor's multi_logloss: 2.59186\n",
      "[67]\tevalset_floor's multi_logloss: 2.59152\n",
      "[68]\tevalset_floor's multi_logloss: 2.5912\n",
      "[69]\tevalset_floor's multi_logloss: 2.59088\n",
      "[70]\tevalset_floor's multi_logloss: 2.59059\n",
      "[71]\tevalset_floor's multi_logloss: 2.59054\n",
      "[72]\tevalset_floor's multi_logloss: 2.59051\n",
      "[73]\tevalset_floor's multi_logloss: 2.59048\n",
      "[74]\tevalset_floor's multi_logloss: 2.59045\n",
      "[75]\tevalset_floor's multi_logloss: 2.59044\n",
      "[76]\tevalset_floor's multi_logloss: 2.5905\n",
      "[77]\tevalset_floor's multi_logloss: 2.59057\n",
      "[78]\tevalset_floor's multi_logloss: 2.59064\n",
      "[79]\tevalset_floor's multi_logloss: 2.59071\n",
      "[80]\tevalset_floor's multi_logloss: 2.59078\n",
      "[81]\tevalset_floor's multi_logloss: 2.59083\n",
      "[82]\tevalset_floor's multi_logloss: 2.59088\n",
      "[83]\tevalset_floor's multi_logloss: 2.59094\n",
      "[84]\tevalset_floor's multi_logloss: 2.59099\n",
      "[85]\tevalset_floor's multi_logloss: 2.59105\n",
      "[86]\tevalset_floor's multi_logloss: 2.59118\n",
      "[87]\tevalset_floor's multi_logloss: 2.59131\n",
      "[88]\tevalset_floor's multi_logloss: 2.59144\n",
      "[89]\tevalset_floor's multi_logloss: 2.59158\n",
      "[90]\tevalset_floor's multi_logloss: 2.59171\n",
      "[91]\tevalset_floor's multi_logloss: 2.59181\n",
      "[92]\tevalset_floor's multi_logloss: 2.59192\n",
      "[93]\tevalset_floor's multi_logloss: 2.59203\n",
      "[94]\tevalset_floor's multi_logloss: 2.59214\n",
      "[95]\tevalset_floor's multi_logloss: 2.59225\n",
      "[96]\tevalset_floor's multi_logloss: 2.59232\n",
      "[97]\tevalset_floor's multi_logloss: 2.59238\n",
      "[98]\tevalset_floor's multi_logloss: 2.59245\n",
      "[99]\tevalset_floor's multi_logloss: 2.59252\n",
      "[100]\tevalset_floor's multi_logloss: 2.5926\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[75]\tevalset_floor's multi_logloss: 2.59044\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [3.0, 4.0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-05e73f10a1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_reg_lat_lgb_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgbr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams_lgbr_lat_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_cat_floor_lgb_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams_lgbc_floor_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel_cat_building_lgb_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams_lgbc_building_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel_cat_area_lgb_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams_lgbc_area_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    786\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 788\u001b[1;33m                     \u001b[0meval_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         super(LGBMClassifier, self).fit(X, _y, sample_weight=sample_weight,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36m_encode\u001b[1;34m(values, uniques, encode)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_encode_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36m_encode_numpy\u001b[1;34m(values, uniques, encode)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             raise ValueError(\"y contains previously unseen labels: %s\"\n\u001b[1;32m---> 49\u001b[1;33m                              % str(diff))\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: [3.0, 4.0]"
     ]
    }
   ],
   "source": [
    "lgbr = lgb.LGBMRegressor(**params_lgbr)\n",
    "lgbc = lgb.LGBMClassifier(**params_lgbc)\n",
    "model_reg_long_lgb_fit = lgbr.fit(X = x_train, y = y_train[:, 0], **params_lgbr_long_fit, early_stopping_rounds = 100)\n",
    "model_reg_lat_lgb_fit = lgbr.fit(X = x_train, y = y_train[:, 1], **params_lgbr_lat_fit, early_stopping_rounds = 100)\n",
    "model_cat_floor_lgb_fit = lgbc.fit(X = x_train, y = y_train[:, 2], **params_lgbc_floor_fit, early_stopping_rounds = 100)\n",
    "model_cat_building_lgb_fit = lgbc.fit(X = x_train, y = y_train[:, 3], **params_lgbc_building_fit, early_stopping_rounds = 100)\n",
    "model_cat_area_lgb_fit = lgbc.fit(X = x_train, y = y_train[:, 4], **params_lgbc_area_fit, early_stopping_rounds = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_long_train = model_reg_long_lgb.predict(x_train)\n",
    "predict_long_validate = model_reg_long_lgb.predict(x_validate)\n",
    "predict_lat_train = model_reg_lat_lgb.predict(x_train)\n",
    "predict_lat_validate = model_reg_lat_lgb.predict(x_validate)\n",
    "predict_floor_train = model_cat_floor_lgb.predict(x_train)\n",
    "predict_floor_validate = model_cat_floor_lgb.predict(x_validate)\n",
    "predict_building_train = model_cat_building_lgb.predict(x_train)\n",
    "predict_building_validate = model_cat_building_lgb.predict(x_validate)\n",
    "predict_area_train = model_cat_area_lgb.predict(x_train)\n",
    "predict_area_validate = model_cat_area_lgb.predict(x_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_long_test1 = model_reg_long_lgb.predict(x_test1)\n",
    "predict_lat_test1 = model_reg_lat_lgb.predict(x_test1)\n",
    "predict_floor_test1 = model_cat_floor_lgb.predict(x_test1)\n",
    "predict_building_test1 = model_cat_building_lgb.predict(x_test1)\n",
    "predict_area_test1 = model_cat_area_lgb.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115505e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115510e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115514e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124778e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124781e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124784e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4864931.4921   4864849.3028   4864842.961439 ... 4864790.0184\n",
      " 4864769.4062   4865010.6595  ]\n",
      "[4864932.18652862 4864848.93309248 4864841.04304336 ... 4864840.30335268\n",
      " 4864769.3951732  4865004.18423284]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:, 1])\n",
    "print(predict_lat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7680.1346     -7372.8029     -7377.07826999 ... -7401.9962\n",
      " -7368.3861     -7640.9762    ]\n",
      "[-7675.98644586 -7370.27021826 -7364.01905812 ... -7401.34351258\n",
      " -7365.23751479 -7640.02241201]\n",
      "[-7511.5215     -7535.39365217 -7348.8982     ... -7408.69525072\n",
      " -7445.55787384 -7390.7612    ]\n",
      "[-7513.62171703 -7535.00433142 -7346.12423151 ... -7426.8727567\n",
      " -7426.96422075 -7364.04356366]\n",
      "[0. 4. 0. ... 0. 3. 0.]\n",
      "[0 4 0 ... 0 3 0]\n",
      "[2. 2. 3. ... 2. 0. 3.]\n",
      "[2 2 3 ... 2 0 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:, 0])\n",
    "print(predict_long_train)\n",
    "print(y_validate[:, 0])\n",
    "print(predict_long_validate)\n",
    "print(y_train[:, 2])\n",
    "print(predict_floor_train.argmax(axis = 1))\n",
    "print(y_validate[:, 2])\n",
    "print(predict_floor_validate.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7653.34330145 -7436.55759842 -7633.11873588 -7642.79786687\n",
      " -7676.7537     -7587.7889     -7512.88887257 -7631.74452741\n",
      " -7669.77241616 -7639.64924148 -7358.37957947 -7631.67139819\n",
      " -7314.6117     -7685.4946     -7634.40627116 -7640.18192595\n",
      " -7638.81872605 -7516.16625317 -7644.77762235 -7383.69939217\n",
      " -7636.8284     -7458.8441     -7647.68238911 -7357.39150357\n",
      " -7431.7139     -7521.96922154 -7603.77416454 -7637.94412\n",
      " -7516.657719   -7653.34330145 -7589.36752477 -7641.56309275\n",
      " -7312.58323822 -7638.03351094 -7459.73507911 -7650.25597591\n",
      " -7639.23914656 -7609.20937016 -7456.02104199 -7673.74090851\n",
      " -7332.13425947 -7404.54214198 -7377.06790472 -7602.8439641\n",
      " -7420.76892616 -7345.31695461 -7538.91807875 -7653.57921978\n",
      " -7587.86576667 -7684.86795799 -7321.58209643 -7641.35244\n",
      " -7641.316      -7309.2194     -7355.99039628 -7383.03510326\n",
      " -7417.55512884 -7645.02567237 -7325.19074434 -7559.67807372\n",
      " -7506.96953939 -7616.9783     -7418.37986592 -7299.78651673\n",
      " -7595.974      -7641.34508183 -7662.30661873 -7688.29800091\n",
      " -7394.22637015 -7676.41944    -7429.96611795 -7309.62392747\n",
      " -7688.90760244 -7434.3531     -7386.60597407 -7611.24801024\n",
      " -7331.90009735 -7406.09842188 -7618.08735494 -7316.34810369\n",
      " -7594.0349     -7419.78586577 -7662.30642142 -7592.14951716\n",
      " -7633.8746     -7404.47798449 -7604.70751951 -7589.06621643\n",
      " -7641.01033919 -7315.0805442  -7690.50086667 -7611.4836831\n",
      " -7637.70483357 -7632.1082043  -7522.34711532 -7640.92615\n",
      " -7530.49750986 -7460.28649435 -7603.3359     -7342.73677076\n",
      " -7373.24268719 -7682.39338182 -7676.41944    -7637.55456227\n",
      " -7353.77175714 -7562.62688746 -7594.392463   -7603.3359\n",
      " -7634.47592141 -7324.37475018 -7615.52771818 -7635.62794169\n",
      " -7374.2540875  -7675.14641362 -7516.98801731 -7327.01093172\n",
      " -7638.1938     -7587.53908333 -7645.61007713 -7644.01243109\n",
      " -7682.53929817 -7366.08527868 -7595.18075949 -7665.65401487\n",
      " -7636.89562666 -7309.2194     -7385.79996494 -7460.9975\n",
      " -7329.7303614  -7676.65703885 -7401.58428788 -7661.13608\n",
      " -7516.24374803 -7383.32881727 -7537.02489977 -7435.38833459\n",
      " -7555.55767914 -7601.58873393 -7618.42008151 -7596.90703163\n",
      " -7365.17068046 -7536.8266     -7503.49280242 -7691.74018131\n",
      " -7644.88606819 -7617.69924333 -7424.8335648  -7640.0948\n",
      " -7655.8102     -7535.763      -7636.43874556 -7460.60251205\n",
      " -7609.36276364 -7558.85901384 -7642.55282222 -7339.12377143\n",
      " -7329.0308     -7397.56092565 -7642.14688755 -7658.42627815\n",
      " -7536.09291376 -7541.1410682  -7468.44811215 -7639.47659723\n",
      " -7644.36514611 -7386.2219723  -7422.46892012 -7434.2386\n",
      " -7633.23286411 -7594.1054572  -7562.07136316 -7524.34123984\n",
      " -7635.16969401 -7588.43323626 -7535.2095154  -7345.22848197\n",
      " -7453.81238022 -7422.1096     -7402.40428437 -7592.2349353\n",
      " -7616.53860485 -7652.15286522 -7493.38035002 -7376.93162641\n",
      " -7640.31144952 -7616.89191417 -7345.87224664 -7331.179\n",
      " -7642.56660209 -7638.57466338 -7524.48676248 -7421.48609889\n",
      " -7389.45451714 -7325.68829935 -7371.89846    -7377.52905048\n",
      " -7512.31796745 -7637.72554864 -7541.9829     -7555.42255038\n",
      " -7596.72723128 -7609.15270232 -7391.01192857 -7632.41680527\n",
      " -7414.45028333 -7638.6821105  -7323.65790323 -7634.91499007\n",
      " -7311.872768   -7636.08128211 -7384.3846     -7422.2809\n",
      " -7660.74255472 -7633.8746     -7338.48883333 -7420.65298243\n",
      " -7594.0349     -7397.03390701 -7314.00135458 -7632.18845813\n",
      " -7324.14426316 -7562.74598828 -7595.53920064 -7460.03721595\n",
      " -7631.59079789 -7385.4008     -7405.1024     -7653.34330145\n",
      " -7638.28677719 -7406.17866599 -7506.96953939 -7335.9062\n",
      " -7492.04601387 -7382.05637609 -7656.9162095  -7423.0756\n",
      " -7377.91314345 -7587.19017365 -7636.65400505 -7315.4705\n",
      " -7533.4321     -7497.42807644 -7523.82014787 -7643.81766279\n",
      " -7633.8746     -7498.69736354 -7342.70908956 -7592.57327101\n",
      " -7381.95992024 -7360.69503333 -7317.55049873 -7640.41686663\n",
      " -7592.16400307 -7337.77540847 -7592.5623     -7695.93875493\n",
      " -7589.6618243  -7341.24756843 -7600.12173745 -7337.99990672\n",
      " -7641.44614286 -7595.974      -7607.0766     -7605.14416078\n",
      " -7643.13024248 -7358.48536707 -7636.03333375 -7534.43217116\n",
      " -7456.6495     -7318.78358591 -7408.72009965 -7345.08516991\n",
      " -7526.14470916 -7460.45255321 -7522.97416669 -7432.028511\n",
      " -7638.63771657 -7540.46767945 -7311.43873487 -7662.4472\n",
      " -7433.10597504 -7362.99545364 -7384.3846     -7419.68738149\n",
      " -7673.91343881 -7641.18728971 -7519.58592901 -7634.75926747\n",
      " -7610.5094     -7535.763      -7520.7504728  -7434.27657729\n",
      " -7661.08669793 -7470.4254     -7311.47568755 -7628.39150841\n",
      " -7507.6406     -7536.45222843 -7637.41882623 -7632.97907592\n",
      " -7504.02179983 -7636.78198396 -7385.19852091 -7316.90971591\n",
      " -7628.03855667 -7565.10851016 -7640.7165936  -7641.55958312\n",
      " -7626.97029495 -7642.22313555 -7594.26554123 -7560.3763\n",
      " -7437.5567578  -7637.02140959 -7399.23162387 -7347.22066904\n",
      " -7681.73091457 -7647.7315     -7367.74953334 -7639.3453155\n",
      " -7595.48529756 -7406.62981815 -7593.60236454 -7586.9436861\n",
      " -7642.24656108 -7646.7335     -7554.8974511  -7372.60977803\n",
      " -7649.3392     -7695.93195495 -7342.9724     -7435.5184\n",
      " -7435.97428979 -7432.75504816 -7632.7776     -7596.47407935\n",
      " -7637.00156115 -7681.34618818 -7398.58762857 -7313.08025496\n",
      " -7499.37757407 -7429.60901798 -7424.81234447 -7609.0212\n",
      " -7661.39425594 -7374.30207994 -7318.45618655 -7526.6018356\n",
      " -7616.64529249 -7616.12843347 -7324.75889893 -7382.11742566\n",
      " -7645.19122502 -7407.62730274 -7458.7266     -7641.85357309\n",
      " -7683.79114092 -7513.10449072 -7664.41765113 -7631.48092961\n",
      " -7452.17709703 -7636.39292174 -7512.59364791 -7505.6707226\n",
      " -7603.58481554 -7609.0212     -7554.7819682  -7653.34330145\n",
      " -7453.85408994 -7643.0002     -7407.38420251 -7506.7627\n",
      " -7454.6075916  -7323.82668382 -7641.0339727  -7587.80043\n",
      " -7631.91103339 -7562.7171     -7539.10921258 -7524.7095\n",
      " -7570.55529301 -7646.57608968 -7512.59142773 -7659.83335348\n",
      " -7674.58359754 -7631.76840685 -7652.24195691 -7616.30243891\n",
      " -7594.52429311 -7345.63982624 -7345.67172319 -7636.08133352\n",
      " -7641.7741272  -7647.79099607 -7593.08018465 -7316.94520982\n",
      " -7352.42452346 -7383.34269894 -7639.73585659 -7660.28159443\n",
      " -7674.78528286 -7384.23350577 -7651.99609019 -7409.612\n",
      " -7592.05095434 -7509.80545456 -7675.32093333 -7682.79365\n",
      " -7502.89131736 -7619.01281685 -7526.74751611 -7637.07155531\n",
      " -7502.63204469 -7616.64107582 -7603.83305061 -7457.3606503\n",
      " -7505.877579   -7632.37887963 -7635.38288571 -7405.19453483\n",
      " -7310.51205711 -7315.5419     -7528.23176474 -7535.763\n",
      " -7322.95382504 -7609.0212     -7594.17022658 -7511.7367\n",
      " -7530.67628716 -7338.48883333 -7642.30636127 -7507.29146111\n",
      " -7667.04619502 -7358.67326975 -7609.25457427 -7345.59141627\n",
      " -7309.2194     -7637.5574428  -7664.31364315 -7438.78889728\n",
      " -7331.179      -7634.02642033 -7505.95103848 -7333.79575536\n",
      " -7610.8998     -7338.80721042 -7503.86859069 -7636.8946375\n",
      " -7642.10423034 -7313.80849219 -7525.57069158 -7534.86864577\n",
      " -7405.882      -7646.7335     -7374.30803333 -7512.53907923\n",
      " -7359.50280155 -7537.11086159 -7689.2842     -7319.5494449\n",
      " -7551.11733583 -7673.43529177 -7688.35524582 -7641.68709611\n",
      " -7336.64869168 -7533.08632738 -7367.66560409 -7505.09062347\n",
      " -7366.6345     -7682.07559028 -7634.21146584 -7423.1762\n",
      " -7340.46657898 -7588.03157167 -7632.8345     -7636.67734828\n",
      " -7637.92258811 -7632.2587     -7597.50172979 -7684.45530713\n",
      " -7638.17306341 -7676.89365322 -7393.43514992 -7665.65022802\n",
      " -7632.91792672 -7638.93947694 -7469.37200292 -7616.0996\n",
      " -7507.6706     -7436.43220252 -7372.83090404 -7536.73166805\n",
      " -7523.19860865 -7322.2416     -7616.94885039 -7500.63820273\n",
      " -7632.8345     -7327.40499558 -7644.71326286 -7498.05807198\n",
      " -7504.39731642 -7636.85720649 -7506.83438298 -7680.61785946\n",
      " -7345.8039     -7351.9811253  -7640.01652132 -7424.81234447\n",
      " -7394.51967267 -7673.82993483 -7411.09399339 -7589.17943744\n",
      " -7309.2194     -7638.17866494 -7513.78224363 -7525.81812999\n",
      " -7385.87153572 -7594.12514444 -7642.33404309 -7636.0402\n",
      " -7653.34330145 -7399.78489396 -7586.94481643 -7568.74270715\n",
      " -7375.0865372  -7514.57236937 -7337.79011915 -7585.38997622\n",
      " -7315.55288462 -7633.54917988 -7346.33058926 -7458.87629528\n",
      " -7638.85627681 -7346.61134596 -7639.16957923 -7357.34156637\n",
      " -7515.94655759 -7637.63500148 -7410.58470586 -7645.5584\n",
      " -7637.52033381 -7327.95739753 -7644.84284957 -7434.73639197\n",
      " -7473.79119892 -7466.93455018 -7601.387      -7374.28515854\n",
      " -7616.57226202 -7637.18346462 -7627.5311662  -7587.1353311\n",
      " -7643.18740017 -7404.89764228 -7479.7556    ]\n",
      "[-7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7508.18043277 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.37463795 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7482.3651826  -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7521.14398005 -7522.06718502 -7505.52426968\n",
      " -7489.01451566 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7488.09218546 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7528.44158246 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7496.09141558 -7505.52426968 -7505.52426968 -7526.71256595\n",
      " -7508.41582364 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7518.77342469 -7505.52426968\n",
      " -7505.50194783 -7493.82704595 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7522.14959105 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.47204568 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7487.88286726\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7503.20089484 -7505.52426968 -7530.66981437 -7505.52426968\n",
      " -7528.79064127 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7524.80303436 -7495.63916232 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.43689713 -7505.52426968\n",
      " -7505.52426968 -7531.98011544 -7505.52426968 -7503.80276008\n",
      " -7505.52426968 -7522.1568682  -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7532.32509557 -7526.53065335 -7505.49891665 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7522.42837263 -7528.90544832\n",
      " -7505.52426968 -7505.52426968 -7522.32876779 -7505.52426968\n",
      " -7504.71071336 -7504.54578432 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7496.36058957 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7527.10333161 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7508.4864331  -7505.52426968 -7522.68128726 -7522.33139462\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7504.56702952\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7496.41273292\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.49891665\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7495.77523314 -7505.52426968\n",
      " -7495.73436804 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7528.26587944 -7486.39884378 -7518.78014181 -7505.52426968\n",
      " -7505.52426968 -7512.68075276 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7527.60198505\n",
      " -7504.73606639 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7519.37671914 -7505.49891665 -7526.26550874 -7505.52426968\n",
      " -7505.52426968 -7531.96746577 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7527.04101949 -7505.52426968\n",
      " -7505.52426968 -7520.400899   -7509.69988968 -7505.52426968\n",
      " -7505.52426968 -7505.43121784 -7505.52426968 -7522.06718502\n",
      " -7505.30177269 -7530.98897606 -7505.52426968 -7505.52426968\n",
      " -7504.92241768 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7526.02729863 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7532.32330252\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7504.94008078\n",
      " -7505.52426968 -7495.10802541 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7486.07848814 -7505.52426968 -7505.52426968 -7506.34162052\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7529.90590645\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.49891665 -7505.52426968\n",
      " -7505.52426968 -7512.20997232 -7505.52426968 -7522.06718502\n",
      " -7500.08003882 -7505.52426968 -7511.39128417 -7510.56228885\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7497.74136747 -7505.52426968 -7505.52426968 -7508.54076186\n",
      " -7505.49891665 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7518.21357901 -7530.30304831 -7506.34508602\n",
      " -7505.52426968 -7505.52426968 -7511.36774052 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7503.34386184 -7505.52426968 -7505.52426968\n",
      " -7505.48827077 -7505.52426968 -7503.18332758 -7505.52426968\n",
      " -7497.12099607 -7505.52426968 -7505.52426968 -7505.49891665\n",
      " -7491.87788668 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7518.05492897 -7528.96508417\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7512.77280545\n",
      " -7526.9501477  -7505.52426968 -7505.52426968 -7490.02172485\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7502.74077661 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7495.95218454 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7521.95555599 -7527.1050478\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7510.0789981\n",
      " -7505.52426968 -7530.72425438 -7505.52426968 -7505.52426968\n",
      " -7528.79064127 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7528.38104166 -7505.52426968 -7508.61137131\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.50194783 -7505.52426968\n",
      " -7522.06718502 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7493.91618718 -7505.52426968\n",
      " -7508.41582364 -7505.52426968 -7505.52426968 -7530.1124769\n",
      " -7527.04101949 -7505.52426968 -7505.52426968 -7502.92230416\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7486.39884378\n",
      " -7504.63074369 -7505.52426968 -7506.2595874  -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7510.68722707 -7522.06718502\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7522.18511566\n",
      " -7505.52426968 -7510.56228885 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7482.37165904\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7510.56228885 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7496.41273292\n",
      " -7483.11644583 -7493.91618718 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7505.52426968 -7505.52426968\n",
      " -7505.52426968 -7505.52426968 -7489.0399047 ]\n",
      "[4864920.96325149 4864863.33191387 4864964.77103529 4864924.11173886\n",
      " 4864934.6235     4864986.5422353  4864842.37835919 4864987.55549873\n",
      " 4864935.27048836 4865007.29426405 4864836.87899513 4864981.89816087\n",
      " 4864800.71239999 4864929.1348     4864984.26527621 4865005.7027313\n",
      " 4864999.21356321 4864844.08977152 4864917.96574232 4864782.21645804\n",
      " 4864897.3584     4864852.5069     4864944.16651411 4864763.57069022\n",
      " 4864884.1179     4864949.18407008 4864979.60201216 4864903.53590001\n",
      " 4864894.72674274 4864920.96325149 4864981.48060956 4864922.35505802\n",
      " 4864797.23687493 4864912.46378222 4864824.28287146 4864937.11905396\n",
      " 4864911.93974776 4864978.21404511 4864830.3957034  4864934.39592269\n",
      " 4864822.03264036 4864808.42804322 4864849.18512059 4864981.81268828\n",
      " 4864895.39272882 4864829.31808009 4864919.89244437 4864920.20018422\n",
      " 4864986.5202     4864930.97325628 4864795.51402341 4865015.71328\n",
      " 4865015.5861     4864812.2454     4864761.50566503 4864845.91152185\n",
      " 4864892.83210187 4864929.80102579 4864818.0849553  4864887.49139782\n",
      " 4864864.14937869 4864975.191825   4864893.40437628 4864818.21735282\n",
      " 4864981.2125     4865008.71753647 4864934.36715101 4864928.55749322\n",
      " 4864821.05636429 4864934.71934    4864889.67668548 4864812.3262558\n",
      " 4864932.30080884 4864861.3035     4864834.27704985 4864977.68260902\n",
      " 4864768.16935848 4864801.1732583  4864970.03305794 4864800.41937634\n",
      " 4864981.7684     4864894.25396965 4864938.2719655  4864978.76891258\n",
      " 4864970.4821     4864791.88475664 4864980.74518521 4864985.33224475\n",
      " 4864920.68700804 4864813.3721025  4864927.6996     4864978.02362483\n",
      " 4864955.73177154 4864982.42988779 4864839.18486463 4864922.0629\n",
      " 4864956.8820152  4864810.61321927 4864979.1024     4864754.49906421\n",
      " 4864776.56055188 4864930.83715454 4864934.71933999 4865000.97789346\n",
      " 4864833.20322857 4864867.4305765  4864982.32877039 4864979.1024\n",
      " 4864986.22252179 4864816.7505322  4864981.65892278 4864966.48143258\n",
      " 4864844.566875   4864932.8785124  4864889.15920349 4864784.33352455\n",
      " 4864993.2706     4864986.61385    4864918.17099721 4864947.1886584\n",
      " 4864932.53338664 4864842.56860013 4864982.78289028 4864936.72643714\n",
      " 4864905.42273969 4864812.2454     4864837.27407372 4864809.4587\n",
      " 4864820.94415041 4864933.44839218 4864790.95096072 4864939.10022\n",
      " 4864894.57908299 4864844.7724941  4864918.27673816 4864861.66776533\n",
      " 4864902.01768344 4864980.27329621 4864974.84117645 4864983.12509262\n",
      " 4864841.70826003 4864933.9358     4864884.68283767 4864927.78429872\n",
      " 4864948.47653335 4864977.88487317 4864875.46095676 4865000.9183\n",
      " 4864938.3647     4864917.7352     4864991.85664854 4864810.9351958\n",
      " 4864977.37505454 4864871.12401172 4864947.72018333 4864756.6553\n",
      " 4864826.0354     4864783.09017798 4865017.3646842  4864938.04481003\n",
      " 4864929.25138058 4864914.20066476 4864818.48617839 4865001.20051871\n",
      " 4864929.74060647 4864777.33445893 4864895.51630088 4864861.24090001\n",
      " 4864970.39530986 4864981.95862272 4864866.13883356 4864933.40124106\n",
      " 4864966.32790716 4864980.97521445 4864948.0904118  4864829.62455846\n",
      " 4864828.58242426 4864878.7878     4864806.40366393 4864978.25980826\n",
      " 4864978.73758704 4864925.90626959 4864880.25790364 4864773.19664303\n",
      " 4864957.97150049 4864979.16187527 4864749.66572636 4864820.6688\n",
      " 4865016.27466046 4864906.72651549 4864933.91020315 4864894.55921288\n",
      " 4864835.60361653 4864817.85897153 4864849.81814    4864848.80872252\n",
      " 4864842.62307999 4864898.55458955 4864921.10899998 4864885.77888166\n",
      " 4864982.00636835 4864978.04042304 4864836.71427143 4864988.14721207\n",
      " 4864788.4646     4864911.54200328 4864819.07894952 4864973.19002133\n",
      " 4864804.96182384 4864904.21843358 4864842.657      4864878.47640001\n",
      " 4864935.18217665 4864970.4821     4864824.72436667 4864895.87743876\n",
      " 4864981.7684     4864828.82134055 4864798.13277306 4864981.71495641\n",
      " 4864818.24437466 4864889.69580968 4864982.7153957  4864814.48869958\n",
      " 4864958.99603525 4864779.7293     4864786.6725     4864920.9632515\n",
      " 4864906.93779835 4864788.49169098 4864864.14937869 4864829.8497\n",
      " 4864884.06667608 4864845.80459397 4864937.62763416 4864896.08399999\n",
      " 4864849.99195018 4864986.32066996 4864904.935863   4864800.978\n",
      " 4864940.184      4864857.61641572 4864933.54700628 4864930.43217081\n",
      " 4864970.4821     4864868.41674979 4864755.79171302 4864984.10219659\n",
      " 4864844.97940046 4864837.04416667 4864794.97420173 4865010.64924526\n",
      " 4864978.94446059 4864826.50735658 4864981.852      4864931.80820487\n",
      " 4864985.44919319 4864755.52699914 4864980.27976536 4864825.93929428\n",
      " 4865016.04031428 4864981.2125     4864977.6918     4864978.71192022\n",
      " 4864946.6501597  4864840.33160573 4864898.45175669 4864938.17260027\n",
      " 4864823.1814     4864814.57736108 4864795.06533031 4864830.81747028\n",
      " 4864933.67130991 4864816.03854128 4864947.54872279 4864873.58407945\n",
      " 4864906.6674506  4864920.0486662  4864809.67020324 4864933.7721\n",
      " 4864881.30812171 4864764.87255048 4864842.657      4864893.98148622\n",
      " 4864933.75290291 4865006.345573   4864949.82454727 4864966.11017487\n",
      " 4864982.554      4864917.7352     4864854.41564837 4864868.21566241\n",
      " 4864935.22914026 4864814.6952     4864813.082901   4864895.9758312\n",
      " 4864862.8718     4864918.04950621 4864955.69325027 4864965.22725023\n",
      " 4864864.9694765  4864993.8495243  4864781.10850575 4864814.10899933\n",
      " 4864950.3652254  4864871.92161102 4865008.04813466 4864919.86822476\n",
      " 4864979.47950319 4865015.53803664 4864983.82597524 4864888.2621\n",
      " 4864863.47804029 4864993.48622219 4864790.88857294 4864748.01548531\n",
      " 4864931.85113189 4864945.48879999 4864840.95806666 4865007.27142334\n",
      " 4864982.7225867  4864801.05209064 4864984.36972274 4864985.92201844\n",
      " 4865014.26575393 4864925.0598     4864884.97577409 4864843.30257427\n",
      " 4864936.5404     4864931.43318821 4864756.1903     4864877.3166\n",
      " 4864862.25929523 4864882.98545034 4864964.7989     4864983.24845654\n",
      " 4864905.61976625 4864931.88164595 4864787.04535715 4864807.35395044\n",
      " 4864867.72338258 4864878.44714245 4864862.99215681 4864977.473\n",
      " 4864936.59814304 4864846.5337223  4864800.60817568 4864935.08431023\n",
      " 4864976.24902887 4864977.14325118 4864817.93290819 4864846.09787826\n",
      " 4864945.74851699 4864799.43961044 4864832.1627     4864920.27497866\n",
      " 4864930.40131101 4864943.86961973 4864935.77067534 4864894.79584764\n",
      " 4864836.02503973 4864951.45692939 4864841.58790272 4864864.85842014\n",
      " 4864979.52465928 4864977.473      4864885.2475042  4864920.96325149\n",
      " 4864829.53558628 4864930.3138     4864789.02514714 4864850.7019\n",
      " 4864860.38695715 4864818.15330008 4865013.5498822  4864986.53893\n",
      " 4864965.83158611 4864865.95770001 4864919.22042121 4864952.5653\n",
      " 4864891.58683523 4864931.61135403 4864841.90361119 4864938.75064922\n",
      " 4864934.30673881 4864982.25775113 4864922.75925641 4864979.77388835\n",
      " 4864983.70255453 4864756.39679538 4864750.43672313 4864901.11564377\n",
      " 4865010.4449637  4864932.28614492 4864982.77095937 4864814.8214286\n",
      " 4864764.22363806 4864841.28229121 4864958.07349949 4864937.49376101\n",
      " 4864934.17609074 4864775.33266591 4864934.5197158  4864797.1854\n",
      " 4864978.45294343 4864852.23113523 4864933.37678888 4864931.40064999\n",
      " 4864864.82647639 4864975.71671874 4864845.53644596 4864908.67238174\n",
      " 4864861.2526387  4864979.35878392 4864981.56046625 4864831.44490372\n",
      " 4864858.42883813 4864988.06522986 4864984.47952857 4864801.23461032\n",
      " 4864812.517018   4864800.84928    4864900.95922117 4864917.7352\n",
      " 4864818.99955844 4864977.473      4864982.24056738 4864841.6472\n",
      " 4864933.70159981 4864824.72436667 4865016.65220891 4864851.37090193\n",
      " 4864936.20052303 4864838.88538284 4864978.3834198  4864830.51192838\n",
      " 4864812.2454     4864905.20627089 4864935.94171298 4864864.00407898\n",
      " 4864820.6688     4864984.09705941 4864867.06913609 4864821.61684779\n",
      " 4864976.9343     4864825.46686278 4864866.92547348 4864897.589675\n",
      " 4865014.82158361 4864797.15897338 4864957.80791549 4864939.48565194\n",
      " 4864808.6372     4864925.0598     4864844.5968     4864841.78358635\n",
      " 4864837.17241508 4864917.99325567 4864928.0484     4864815.5105007\n",
      " 4864897.77481421 4864934.63670857 4864929.22818364 4865005.41568679\n",
      " 4864827.08814198 4864937.62961685 4864841.80972032 4864856.0344684\n",
      " 4864840.33950001 4864931.22438543 4864984.24410475 4864896.14090001\n",
      " 4864758.1468522  4864985.51979202 4864964.8305     4864993.15039005\n",
      " 4865000.02050305 4864981.12309999 4864981.68444115 4864930.91572201\n",
      " 4864906.76625671 4864933.95646943 4864836.73207044 4864936.34478932\n",
      " 4864969.74029364 4864952.24733173 4864815.9441304  4864980.4364\n",
      " 4864862.88730001 4864838.93088482 4864844.76070332 4864934.23035093\n",
      " 4864951.85325627 4864822.2687     4864979.69261693 4864864.06216758\n",
      " 4864964.8305     4864783.04320289 4864929.17910975 4864856.55098058\n",
      " 4864886.16043367 4864994.49528199 4864851.06706626 4864932.35872436\n",
      " 4864828.7827     4864843.49334408 4864913.7152537  4864862.99215681\n",
      " 4864787.69922358 4864933.49043174 4864792.77166128 4864985.27066531\n",
      " 4864812.2454     4864997.62650328 4864841.4376922  4864845.29862264\n",
      " 4864839.97669286 4864982.07401112 4865014.12730622 4864966.5763\n",
      " 4864920.96325149 4864787.08464092 4864985.8591874  4864876.67910122\n",
      " 4864855.11192198 4864843.12704032 4864763.04415412 4864986.51705366\n",
      " 4864800.82947692 4864964.76870875 4864749.64588937 4864824.62211031\n",
      " 4865010.3999622  4864756.2509207  4865000.82262034 4864767.2139202\n",
      " 4864843.77654189 4864904.55095181 4864793.27600547 4864949.40869999\n",
      " 4864993.58200269 4864782.540322   4864948.67153956 4864855.66360599\n",
      " 4864866.4820823  4864820.44418602 4864979.6612     4864845.82386293\n",
      " 4864979.55970858 4864993.45257394 4864979.5454925  4864985.99605849\n",
      " 4865004.92336985 4864789.51983872 4864895.5038    ]\n",
      "[4864920.60199096 4864920.60199096 4864920.60199096 4864920.99761596\n",
      " 4864920.60199096 4864920.41801135 4864920.60199096 4864920.60199096\n",
      " 4864920.99761596 4864920.60199096 4864920.57537765 4864920.41801135\n",
      " 4864920.38574722 4864920.60199096 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.46537367 4864920.99761596 4864920.46297687\n",
      " 4864920.99761596 4864920.60199096 4864920.60199096 4864920.51605642\n",
      " 4864920.60199096 4864926.03054916 4864920.45444112 4864920.99761596\n",
      " 4864920.53509109 4864920.60199096 4864925.5486496  4864920.99761596\n",
      " 4864920.36216068 4864920.99761596 4864920.60199096 4864920.99761596\n",
      " 4864920.99761596 4864920.60199096 4864920.60199096 4864920.99761596\n",
      " 4864920.55599806 4864920.60199096 4864920.57420118 4864920.60199096\n",
      " 4864920.60199096 4864920.50232618 4864927.71702365 4864920.60199096\n",
      " 4864920.54852453 4864920.60199096 4864920.51605642 4864920.60199096\n",
      " 4864920.60199096 4864920.47796807 4864920.51605642 4864920.38845371\n",
      " 4864920.61997798 4864920.60199096 4864920.54614028 4864930.60057411\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.36216068\n",
      " 4864920.60199096 4864920.60199096 4864920.99761596 4864920.60199096\n",
      " 4864920.51605642 4864920.60199096 4864920.60199096 4864920.47796807\n",
      " 4864920.60199096 4864920.60199096 4864920.51885396 4864920.60199096\n",
      " 4864920.60199096 4864920.51605642 4864920.60199096 4864920.57537765\n",
      " 4864920.54852453 4864920.54852453 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.51605642 4864920.54852453 4864920.41801135\n",
      " 4864920.99761596 4864920.47796807 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.60199096\n",
      " 4864930.92493135 4864920.60199096 4864920.60199096 4864920.46258998\n",
      " 4864920.46258998 4864920.99761596 4864920.99761596 4864920.60199096\n",
      " 4864920.55599806 4864886.64276547 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.46297687 4864920.60199096 4864920.60199096\n",
      " 4864920.39178407 4864920.60199096 4864920.60199096 4864920.38574722\n",
      " 4864920.60199096 4864925.73106184 4864920.99761596 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.54852453 4864920.60199096\n",
      " 4864920.99761596 4864920.46297687 4864920.41801135 4864920.60199096\n",
      " 4864920.55599806 4864920.60199096 4864920.494385   4864920.99761596\n",
      " 4864920.61997798 4864920.60199096 4864917.07799867 4864920.60199096\n",
      " 4864919.80348618 4864920.60199096 4864920.60199096 4864920.60199096\n",
      " 4864920.48891402 4864933.14774963 4864920.61997798 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.54852453 4864920.60199096\n",
      " 4864920.99761596 4864918.47362565 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864887.66346904 4864920.60199096 4864920.51605642\n",
      " 4864920.55727618 4864920.51605642 4864920.60199096 4864920.99761596\n",
      " 4864887.09369313 4864887.60464483 4864920.60199096 4864920.60199096\n",
      " 4864920.99761596 4864920.51605642 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.41801135 4864887.09369313 4864925.06945681\n",
      " 4864920.60199096 4864925.73106184 4864886.70210978 4864920.57576368\n",
      " 4864920.60199096 4864920.61997798 4864920.38574722 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.53509109 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.54614028\n",
      " 4864920.41801135 4864920.99761596 4864925.37027135 4864920.60199096\n",
      " 4864920.60199096 4864920.50716109 4864920.57576368 4864920.60199096\n",
      " 4864920.60199096 4864920.99761596 4864921.14786966 4864920.25286345\n",
      " 4864920.60199096 4864920.60199096 4864920.51885396 4864920.60199096\n",
      " 4864920.51605642 4864920.99761596 4864920.54852453 4864920.60199096\n",
      " 4864920.51605642 4864920.99761596 4864920.60199096 4864920.56651154\n",
      " 4864920.99761596 4864920.60199096 4864920.54614028 4864920.61997798\n",
      " 4864920.60199096 4864920.60199096 4864920.46297687 4864920.60199096\n",
      " 4864920.60199096 4864893.79247707 4864920.41801135 4864920.57537765\n",
      " 4864920.60199096 4864920.57576368 4864920.57537765 4864920.60199096\n",
      " 4864920.99761596 4864920.57537765 4864894.48850137 4864920.50716109\n",
      " 4864920.56651154 4864920.60199096 4864920.60199096 4864920.54852453\n",
      " 4864920.60434541 4864920.41801135 4864920.60199096 4864920.46297687\n",
      " 4864925.78700992 4864920.60199096 4864887.66398914 4864920.99761596\n",
      " 4864920.60199096 4864920.51710407 4864920.51605642 4864920.60199096\n",
      " 4864920.60199096 4864920.55599806 4864920.46297687 4864920.60199096\n",
      " 4864920.60199096 4864920.51605642 4864920.60199096 4864920.60199096\n",
      " 4864925.40831998 4864920.60199096 4864920.60199096 4864920.57576368\n",
      " 4864920.60199096 4864920.54852453 4864920.60199096 4864924.5769749\n",
      " 4864920.99761596 4864920.52191122 4864920.46537367 4864933.47623694\n",
      " 4864920.60199096 4864920.50716109 4864920.51605642 4864920.38574722\n",
      " 4864851.80050445 4864920.57537765 4864926.23531569 4864920.60199096\n",
      " 4864920.99761596 4864919.57025727 4864920.58027618 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.51710407\n",
      " 4864920.60199096 4864920.41801135 4864925.33151726 4864920.60199096\n",
      " 4864920.60199096 4864887.61211414 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.48891402 4864920.86099867\n",
      " 4864920.60199096 4864916.43334128 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.51605642 4864920.58027618\n",
      " 4864920.60199096 4864887.60464483 4864920.60199096 4864920.99761596\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864887.03434881\n",
      " 4864920.60199096 4864920.60199096 4864920.46297687 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.58054047 4864920.60199096\n",
      " 4864920.60199096 4864920.57537765 4864920.60199096 4864925.73106184\n",
      " 4864920.60199096 4864920.60199096 4864887.21306148 4864920.60199096\n",
      " 4864920.60199096 4864920.99761596 4864920.38574722 4864920.60199096\n",
      " 4864920.60199096 4864920.61997798 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.46297687 4864920.46297687\n",
      " 4864920.60199096 4864920.60199096 4864920.54852453 4864924.41324886\n",
      " 4864920.99761596 4864920.60199096 4864920.51605642 4864851.83664455\n",
      " 4864920.60199096 4864920.60199096 4864920.46297687 4864920.57576368\n",
      " 4864920.99761596 4864920.57537765 4864920.60199096 4864920.99761596\n",
      " 4864920.60199096 4864927.68556729 4864920.60199096 4864920.86099867\n",
      " 4864920.61997798 4864920.60199096 4864888.29234602 4864920.60199096\n",
      " 4864920.33707412 4864920.60199096 4864887.60464483 4864920.60199096\n",
      " 4864920.60199096 4864920.99761596 4864920.51605642 4864920.60199096\n",
      " 4864920.60199096 4864920.48891402 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864886.64276547 4864920.9898509  4864932.79707556\n",
      " 4864887.60464483 4864920.99761596 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.99761596\n",
      " 4864920.60199096 4864920.51710407 4864920.51710407 4864920.47796807\n",
      " 4864920.38574722 4864920.60199096 4864920.60199096 4864920.99761596\n",
      " 4864920.60199096 4864920.51605642 4864920.99761596 4864920.51605642\n",
      " 4864920.54852453 4864920.44408601 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.99761596\n",
      " 4864920.60199096 4864920.60199096 4864920.41801135 4864920.60199096\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.51605642\n",
      " 4864920.47796807 4864920.46297687 4864919.49701243 4864916.53698838\n",
      " 4864920.51605642 4864920.60199096 4864920.41801135 4864920.60199096\n",
      " 4864887.18235546 4864920.54614028 4864920.60199096 4864888.29234602\n",
      " 4864920.60199096 4864920.57576368 4864920.60199096 4864920.54614028\n",
      " 4864920.47796807 4864920.99761596 4864920.99761596 4864920.60199096\n",
      " 4864920.54614028 4864920.60199096 4864920.60199096 4864920.50716109\n",
      " 4864920.60199096 4864920.30493441 4864920.61997798 4864920.60199096\n",
      " 4864920.60199096 4864920.54614028 4864928.04638629 4864925.77883015\n",
      " 4864920.51605642 4864921.14791242 4864920.51945852 4864920.46537367\n",
      " 4864920.57537765 4864919.53073531 4864920.60199096 4864920.46297687\n",
      " 4864887.66398914 4864920.60199096 4864920.99761596 4864920.60199096\n",
      " 4864920.52191122 4864932.49723178 4864920.57576368 4864920.60199096\n",
      " 4864920.60434541 4864920.60199096 4864920.60199096 4864920.57368078\n",
      " 4864920.38574722 4864925.80473186 4864920.60199096 4864920.60199096\n",
      " 4864920.41801135 4864920.60199096 4864920.44408601 4864920.99761596\n",
      " 4864920.86099867 4864920.60199096 4864920.55599806 4864920.99761596\n",
      " 4864920.60199096 4864920.60199096 4864920.57537765 4864920.60199096\n",
      " 4864920.38948074 4864920.60199096 4864920.60434541 4864924.978381\n",
      " 4864926.08577077 4864920.50716109 4864920.60199096 4864920.61997798\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.60199096\n",
      " 4864920.61997798 4864920.60199096 4864920.60199096 4864920.99761596\n",
      " 4864920.53598359 4864920.60199096 4864920.60199096 4864920.60199096\n",
      " 4864920.51605642 4864920.60199096 4864920.57537765 4864920.54852453\n",
      " 4864920.47796807 4864920.60199096 4864920.60199096 4864920.46537367\n",
      " 4864920.55589761 4864920.60199096 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.50932618 4864920.60199096 4864887.03434881\n",
      " 4864920.60434541 4864920.60199096 4864920.51605642 4864920.54852453\n",
      " 4864920.54614028 4864920.60199096 4864920.51605642 4864920.60199096\n",
      " 4864920.60199096 4864920.57537765 4864920.60199096 4864920.51605642\n",
      " 4864920.46537367 4864920.99761596 4864920.51605642 4864920.60199096\n",
      " 4864920.60199096 4864920.55589761 4864920.60199096 4864920.61997798\n",
      " 4864920.60199096 4864920.57537765 4864920.60199096 4864920.60434541\n",
      " 4864920.60199096 4864920.60199096 4864920.60199096 4864920.60199096\n",
      " 4864920.60199096 4864920.57537765 4864920.61997798]\n",
      "[2. 0. 1. 2. 2. 2. 2. 2. 1. 1. 1. 3. 0. 0. 2. 1. 2. 1. 2. 1. 2. 0. 1. 4.\n",
      " 3. 1. 1. 0. 2. 2. 1. 0. 1. 1. 0. 1. 1. 1. 3. 1. 1. 1. 0. 1. 0. 3. 3. 2.\n",
      " 2. 2. 4. 2. 2. 1. 3. 3. 2. 2. 1. 3. 1. 1. 1. 1. 1. 1. 1. 1. 4. 1. 2. 1.\n",
      " 0. 3. 1. 2. 4. 4. 1. 4. 1. 2. 0. 1. 2. 2. 2. 2. 0. 1. 2. 2. 0. 2. 2. 3.\n",
      " 3. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 0. 3. 3. 0. 2. 1. 0. 2. 2. 2.\n",
      " 3. 3. 1. 3. 0. 1. 2. 1. 1. 2. 0. 2. 3. 1. 1. 0. 2. 1. 0. 1. 2. 3. 3. 3.\n",
      " 1. 1. 3. 1. 1. 1. 2. 2. 1. 3. 2. 2. 3. 4. 3. 1. 1. 2. 3. 1. 1. 4. 2. 2.\n",
      " 2. 2. 1. 1. 2. 3. 1. 4. 1. 3. 1. 1. 1. 2. 2. 3. 2. 1. 3. 1. 3. 2. 1. 0.\n",
      " 1. 1. 4. 2. 1. 1. 0. 3. 2. 2. 1. 1. 4. 2. 3. 1. 0. 2. 1. 2. 1. 2. 1. 3.\n",
      " 1. 1. 1. 2. 4. 1. 3. 3. 0. 1. 3. 2. 2. 3. 1. 0. 2. 1. 3. 2. 1. 3. 0. 1.\n",
      " 1. 0. 1. 2. 2. 1. 2. 0. 1. 1. 1. 0. 1. 4. 0. 1. 2. 4. 1. 2. 2. 1. 0. 0.\n",
      " 1. 3. 3. 2. 0. 1. 2. 0. 1. 3. 1. 1. 2. 1. 1. 2. 2. 3. 1. 2. 2. 3. 1. 1.\n",
      " 0. 1. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 0. 2. 1. 2. 2. 1. 1. 1.\n",
      " 1. 1. 1. 3. 3. 3. 1. 2. 1. 4. 1. 1. 1. 3. 1. 1. 1. 0. 0. 2. 2. 2. 0. 1.\n",
      " 3. 1. 1. 2. 1. 3. 2. 1. 0. 4. 4. 1. 0. 2. 1. 2. 1. 3. 1. 0. 2. 3. 2. 1.\n",
      " 3. 0. 1. 1. 2. 3. 0. 2. 1. 0. 4. 1. 2. 2. 2. 1. 3. 1. 2. 3. 1. 2. 1. 3.\n",
      " 0. 2. 2. 1. 1. 3. 3. 2. 1. 2. 2. 3. 1. 1. 0. 1. 2. 4. 0. 1. 1. 1. 3. 1.\n",
      " 2. 1. 2. 2. 2. 2. 3. 3. 1. 1. 2. 1. 1. 1. 1. 1. 4. 1. 3. 2. 1. 1. 3. 1.\n",
      " 2. 4. 3. 1. 1. 3. 1. 2. 1. 3. 1. 0. 1. 2. 2. 3. 2. 1. 1. 1. 2. 1. 1. 2.\n",
      " 3. 1. 1. 3. 1. 2. 1. 0. 3. 2. 2. 1. 1. 3. 1. 3. 1. 3. 1. 1. 3. 2. 1. 1.\n",
      " 1. 0. 1. 1. 1. 2. 2. 1. 1. 0. 1. 1. 1. 0. 2. 2. 2. 2. 3. 0. 0. 3. 2. 1.\n",
      " 0. 0. 3. 2. 1. 1. 3. 1. 1. 1. 1. 2. 0. 1. 3. 2. 2. 3. 1. 1. 1. 1. 3. 0.\n",
      " 1. 1. 3. 0. 0. 4. 2. 2. 1. 2. 4. 1. 2. 1. 1. 3. 0. 2. 1. 1. 1. 1. 2. 1.\n",
      " 1. 3. 2.]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 4 2 2 2 2 4 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2\n",
      " 4 2 2 2 2 2 2 2 2 2 2 2 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_test1[:, 0])\n",
    "print(predict_long_test1)\n",
    "print(y_test1[:, 1])\n",
    "print(predict_lat_test1)\n",
    "print(y_test1[:, 2])\n",
    "print(predict_floor_test1.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_floor_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_floor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_building_train = []\n",
    "error_building_validate = []\n",
    "error_floor_train = []\n",
    "error_floor_validate = []\n",
    "error_area_train = []\n",
    "error_area_validate = []\n",
    "predict_floor_argmax_train = predict_floor_train.argmax(axis = 1)\n",
    "predict_floor_argmax_validate = predict_floor_validate.argmax(axis = 1)\n",
    "predict_building_argmax_train = predict_building_train.argmax(axis = 1)\n",
    "predict_building_argmax_validate = predict_building_validate.argmax(axis = 1)\n",
    "predict_area_argmax_train = predict_area_train.argmax(axis = 1)\n",
    "predict_area_argmax_validate = predict_area_validate.argmax(axis = 1)\n",
    "for i in range(len(predict_floor_train)):\n",
    "    if predict_floor_argmax_train[i] != y_train[i, 2]:\n",
    "        error_floor_train.append(i)\n",
    "    if predict_building_argmax_train[i] != y_train[i, 3]:\n",
    "        error_building_train.append(i)\n",
    "    if predict_area_argmax_train[i] != y_train[i, 4]:\n",
    "        error_area_train.append(i)\n",
    "for i in range(len(predict_floor_validate)):\n",
    "    if predict_floor_argmax_validate[i] != y_validate[i, 2]:\n",
    "        error_floor_validate.append(i)\n",
    "    if predict_building_argmax_validate[i] != y_validate[i, 3]:\n",
    "        error_building_validate.append(i)\n",
    "    if predict_area_argmax_validate[i] != y_validate[i, 4]:\n",
    "        error_area_validate.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_rate_floor_train 0.01310509154752947\n",
      "error_rate_floor_validate 0.004075746175068974\n",
      "error_rate_building_train 0.0007524454477050414\n",
      "error_rate_buildilng_validate 0.00043892651116127413\n",
      "error_rate_area_train 0.023200401304238776\n",
      "error_rate_area_validate 0.031093279839518557\n"
     ]
    }
   ],
   "source": [
    "error_rate_floor_train = len(error_floor_train) / len(predict_floor_train)\n",
    "print('error_rate_floor_train', error_rate_floor_train)\n",
    "error_rate_floor_validate = len(error_floor_validate) / len(predict_floor_train)\n",
    "print('error_rate_floor_validate', error_rate_floor_validate)\n",
    "error_rate_building_train = len(error_building_train) / len(predict_building_train)\n",
    "print('error_rate_building_train', error_rate_building_train)\n",
    "error_rate_building_validate = len(error_building_validate) / len(predict_building_train)\n",
    "print('error_rate_buildilng_validate', error_rate_building_validate)\n",
    "error_rate_area_train = len(error_area_train) / len(predict_area_train)\n",
    "print('error_rate_area_train', error_rate_area_train)\n",
    "error_rate_area_validate = len(error_area_validate) / len(predict_area_validate)\n",
    "print('error_rate_area_validate', error_rate_area_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15948\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_long_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_mean_distance_train:  8.931841732489868\n",
      "error_max_distance_train:  175.23642677552965\n",
      "error_min_distance_train:  0.03052850093669021\n",
      "error_std_distance_train:  10.884696354216155\n",
      "error_var_distance_train:  118.47661472348646\n",
      "error_mean_distance_validate:  9.387741998798987\n",
      "error_max_distance_validate:  150.93144848482376\n",
      "error_min_distance_validate:  0.1085996046393881\n",
      "error_std_distance_validate:  11.132779555018809\n",
      "error_var_distance_validate:  123.93878062064479\n"
     ]
    }
   ],
   "source": [
    "error_distance_train = []\n",
    "error_distance_validate = []\n",
    "for i in range(len(predict_long_train)):\n",
    "    error_distance_train.append(euclidean_distance(predict_lat_train[i], predict_long_train[i], y_train[i, 1], y_train[i, 0]))\n",
    "for i in range(len(predict_long_validate)):\n",
    "    error_distance_validate.append(euclidean_distance(predict_lat_validate[i], predict_long_validate[i], y_validate[i, 1], y_validate[i, 0]))\n",
    "error_mean_distance_train = np.mean(np.stack(error_distance_train)).item()\n",
    "error_max_distance_train = np.max(np.stack(error_distance_train)).item()\n",
    "error_min_distance_train = np.min(np.stack(error_distance_train)).item()\n",
    "error_std_distance_train = np.std(np.stack(error_distance_train)).item()\n",
    "error_var_distance_train = np.var(np.stack(error_distance_train)).item()\n",
    "print('error_mean_distance_train: ', error_mean_distance_train)\n",
    "print('error_max_distance_train: ', error_max_distance_train)\n",
    "print('error_min_distance_train: ', error_min_distance_train)\n",
    "print('error_std_distance_train: ', error_std_distance_train)\n",
    "print('error_var_distance_train: ', error_var_distance_train)\n",
    "error_mean_distance_validate = np.mean(np.stack(error_distance_validate)).item()\n",
    "error_max_distance_validate = np.max(np.stack(error_distance_validate)).item()\n",
    "error_min_distance_validate = np.min(np.stack(error_distance_validate)).item()\n",
    "error_std_distance_validate = np.std(np.stack(error_distance_validate)).item()\n",
    "error_var_distance_validate = np.var(np.stack(error_distance_validate)).item()\n",
    "print('error_mean_distance_validate: ', error_mean_distance_validate)\n",
    "print('error_max_distance_validate: ', error_max_distance_validate)\n",
    "print('error_min_distance_validate: ', error_min_distance_validate)\n",
    "print('error_std_distance_validate: ', error_std_distance_validate)\n",
    "print('error_var_distance_validate: ', error_var_distance_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_building_test1 = []\n",
    "error_floor_test1 = []\n",
    "error_area_test1 = []\n",
    "predict_floor_argmax_test1 = predict_floor_test1.argmax(axis = 1)\n",
    "predict_building_argmax_test1 = predict_building_test1.argmax(axis = 1)\n",
    "predict_area_argmax_test1 = predict_area_test1.argmax(axis = 1)\n",
    "for i in range(len(predict_floor_test1)):\n",
    "    if predict_floor_argmax_test1[i] != y_test1[i, 2]:\n",
    "        error_floor_test1.append(i)\n",
    "    if predict_building_argmax_test1[i] != y_test1[i, 3]:\n",
    "        error_building_test1.append(i)\n",
    "    if predict_area_argmax_test1[i] != y_test1[i, 4]:\n",
    "        error_area_test1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_rate_floor_test1 0.6954954954954955\n",
      "error_rate_building_test1 0.7387387387387387\n",
      "error_rate_area_test1 0.8450450450450451\n"
     ]
    }
   ],
   "source": [
    "error_rate_floor_test1 = len(error_floor_test1) / len(predict_floor_test1)\n",
    "print('error_rate_floor_test1', error_rate_floor_test1)\n",
    "error_rate_building_test1 = len(error_building_test1) / len(predict_building_test1)\n",
    "print('error_rate_building_test1', error_rate_building_test1)\n",
    "error_rate_area_test1 = len(error_area_test1) / len(predict_area_test1)\n",
    "print('error_rate_area_test1', error_rate_area_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_mean_distance_test1:  132.23746685466034\n",
      "error_max_distance_test1:  220.51688028049762\n",
      "error_min_distance_test1:  1.4316358435594734\n",
      "error_std_distance_test1:  59.66302191843922\n",
      "error_var_distance_test1:  3559.676184440159\n"
     ]
    }
   ],
   "source": [
    "error_distance_test1 = []\n",
    "for i in range(len(predict_long_test1)):\n",
    "    error_distance_test1.append(euclidean_distance(predict_lat_test1[i], predict_long_test1[i], y_test1[i, 1], y_test1[i, 0]))\n",
    "error_mean_distance_test1 = np.mean(np.stack(error_distance_test1)).item()\n",
    "error_max_distance_test1 = np.max(np.stack(error_distance_test1)).item()\n",
    "error_min_distance_test1 = np.min(np.stack(error_distance_test1)).item()\n",
    "error_std_distance_test1 = np.std(np.stack(error_distance_test1)).item()\n",
    "error_var_distance_test1 = np.var(np.stack(error_distance_test1)).item()\n",
    "print('error_mean_distance_test1: ', error_mean_distance_test1)\n",
    "print('error_max_distance_test1: ', error_max_distance_test1)\n",
    "print('error_min_distance_test1: ', error_min_distance_test1)\n",
    "print('error_std_distance_test1: ', error_std_distance_test1)\n",
    "print('error_var_distance_test1: ', error_var_distance_test1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
