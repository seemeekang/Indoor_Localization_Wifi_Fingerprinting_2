{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "\n",
    "class UJIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train = True, transform = None, target_transform = None, download = False):\n",
    "        self.root = root\n",
    "        dir_path = self.root + '/UJIndoorLoc'\n",
    "        zip_path = self.root + '/uji_uil.zip'\n",
    "        dataset_training_file = dir_path + '/trainingData.csv'\n",
    "        dataset_validation_file = dir_path + '/validationData.csv'\n",
    "        # Load independent variables (WAPs values)\n",
    "        if train:\n",
    "            dataset_file = dataset_training_file\n",
    "        else:\n",
    "            dataset_file = dataset_validation_file\n",
    "        file = open(dataset_file, 'r')\n",
    "        # Load labels\n",
    "        label = file.readline()\n",
    "        label = label.split(',')\n",
    "        # Load independent variables\n",
    "        file_load = np.loadtxt(file, delimiter = ',', skiprows = 1)\n",
    "        #file_load_label = np.loadtxt(file, delimiter = ',')\n",
    "        #data = np.genfromtxt(file, dtype = float, delimiter = ',', names = True)\n",
    "        # RSSI values\n",
    "        self.x = file_load[:, 0 : 520]\n",
    "        # Load dependent variables\n",
    "        self.y = file_load[:, 520 : 524]\n",
    "        # Divide labels into x and y\n",
    "        self.x_label = label[0 : 520]\n",
    "        self.x_label = np.concatenate([self.x_label, label[524: 529]])\n",
    "        self.y_label = label[520 : 524]\n",
    "        # Regularization of independent variables\n",
    "        self.x[self.x == 100] = np.nan    # WAP not detected\n",
    "        self.x = self.x + 104             # Convert into positive values\n",
    "        self.x = self.x / 104             # Regularize into scale between 0 and 1\n",
    "        # Building ID, Space ID, Relative Position, User ID, Phone ID and Timestamp respectively\n",
    "        self.x = np.concatenate([self.x, file_load[:, 524 : 529]], axis = 1)\n",
    "        file.close()\n",
    "        # Reduce the number of dependent variables by combining building number and floor into one variable: area\n",
    "        area = self.y[:, 3] * 5 + self.y[:, 2]\n",
    "        self.y = np.column_stack((self.y, area))\n",
    "    def to_tensor(self):\n",
    "        self.x = torch.from_numpy(self.x).float()\n",
    "        self.y = torch.from_numpy(self.y).float()\n",
    "        self.area = torch.from_numpy(self.area).float()\n",
    "    def nan_to_zero(self):\n",
    "        self.x = np.nan_to_num(self.x)\n",
    "    # Return the target instance (row)\n",
    "    def __getitem__(self, index_row):\n",
    "        return self.x[index_row, :], self.y[index_row, :]\n",
    "    # Return the number of instances (the number of rows)\n",
    "    def __len__(self, dim = 0):\n",
    "        return int(self.x.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance (unit: meter) between two coordinates in EPSG:3857 \n",
    "def euclidean_distance(latitude_1, longitude_1, latitude_2, longitude_2):\n",
    "    return np.sqrt((latitude_1 - latitude_2)**2 + (longitude_1 - longitude_2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset with lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0., 0., 1., 1.],\n",
      "        [0., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros([2,2])\n",
    "print(a)\n",
    "b = torch.ones([2,2])\n",
    "c = torch.cat((a, b), dim = 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "dataset_train = UJIDataset('./data', train = True)\n",
    "dataset_test = UJIDataset('./data', train = False)\n",
    "#gb_train_data = lgb.Dataset("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train.nan_to_zero()\n",
    "#dataset_validate.nan_to_zero()\n",
    "#dataset_test.nan_to_zero()\n",
    "#dataset_train.to_tensor()\n",
    "#dataset_validate.to_tensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37171369e+09]\n",
      " [           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37171410e+09]\n",
      " [           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37171381e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 1.80000000e+01\n",
      "  1.00000000e+01 1.37171092e+09]\n",
      " [           nan            nan            nan ... 1.80000000e+01\n",
      "  1.00000000e+01 1.37171105e+09]\n",
      " [           nan            nan            nan ... 1.80000000e+01\n",
      "  1.00000000e+01 1.37171102e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115505e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115510e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115514e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124778e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124781e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124784e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = sklearn.model_selection.train_test_split(dataset_train.x, dataset_train.y, test_size = 0.2, random_state = 42)\n",
    "x_test1, x_test2, y_test1, y_test2 = sklearn.model_selection.train_test_split(dataset_test.x, dataset_test.y, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 1.00000000e+00\n",
      "  1.40000000e+01 1.37087874e+09]\n",
      " [           nan            nan            nan ... 6.00000000e+00\n",
      "  1.90000000e+01 1.37171987e+09]\n",
      " [           nan            nan            nan ... 1.00000000e+01\n",
      "  8.00000000e+00 1.37172087e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 9.00000000e+00\n",
      "  1.40000000e+01 1.37172008e+09]\n",
      " [           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37172121e+09]\n",
      " [           nan            nan            nan ... 1.00000000e+00\n",
      "  1.40000000e+01 1.37103670e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.2 * len(dataset_train.x))\n",
    "dataset_train_reg_long_lgb = lgb.Dataset(x_train, label = y_train[:, 0])\n",
    "dataset_validate_reg_long_lgb = lgb.Dataset(x_validate, label = y_validate[:, 0])\n",
    "dataset_train_reg_lat_lgb = lgb.Dataset(x_train, label = y_train[:, 1])\n",
    "dataset_validate_reg_lat_lgb = lgb.Dataset(x_validate, label = y_validate[:, 1])\n",
    "dataset_train_cat_floor_lgb = lgb.Dataset(x_train, label = y_train[:, 2])\n",
    "dataset_validate_cat_floor_lgb = lgb.Dataset(x_validate, label = y_validate[:, 2])\n",
    "dataset_train_cat_building_lgb = lgb.Dataset(x_train, label = y_train[:, 3])\n",
    "dataset_validate_cat_building_lgb = lgb.Dataset(x_validate, label = y_validate[:, 3])\n",
    "dataset_train_cat_area_lgb = lgb.Dataset(x_train, label = y_train[:, 4])\n",
    "dataset_validate_cat_area_lgb = lgb.Dataset(x_validate, label = y_validate[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feval(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool\n",
    "def multiclass_score(preds, train_data):\n",
    "    label = train_data.get_label()\n",
    "    return 'multiclass_score', np.mean(label == preds), True\n",
    "\n",
    "def regression_score(preds, train_data):\n",
    "    label = train_data.get_label()\n",
    "    error_distance = []\n",
    "    for i in range(label):\n",
    "        error_distance.append(euclidean_distance(preds[i], 0, label[i], 0))\n",
    "    return 'regression_score', np.mean(error_distance), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_reg = {'learning_rate': 0.001,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 10,\n",
    "              'boosting': 'rf', \n",
    "              'objective': 'regression',\n",
    "              #'metric': 'rmse',\n",
    "              'metric': 'None',\n",
    "              'is_training_metric': True, \n",
    "              'num_leaves': 1440,  \n",
    "              #'feature_fraction': 0.7, \n",
    "              'bagging_fraction': 0.8, \n",
    "              'bagging_freq': 5,\n",
    "              #'early_stopping_round': 100,\n",
    "              #'seed':2018\n",
    "             }\n",
    "params_cat_building = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 300,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 3,\n",
    "              #'metric': 'multi_logloss',\n",
    "              'metric': 'None',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              #'early_stopping_round': 100,\n",
    "             }\n",
    "params_cat_floor = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 300,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 5,\n",
    "              #'metric': 'multi_logloss',\n",
    "              'metric': 'None',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              #'early_stopping_round': 100,\n",
    "             }\n",
    "params_cat_area = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 300,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 15,\n",
    "              #'metric': 'multi_logloss',\n",
    "              'metric': 'None',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              #'early_stopping_round': 100,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg_long_lgb = lgb.train(params_reg, dataset_train_reg_long_lgb, dataset_validate_reg_long_lgb, feval = regression_score, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_reg_lat_lgb = lgb.train(params_reg, dataset_train_reg_lat_lgb, dataset_validate_reg_lat_lgb, feval = regression_score, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_cat_floor_lgb = lgb.train(params_cat_floor, dataset_train_cat_floor_lgb, dataset_validate_cat_floor_lgb, feval = multiclass_score, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_cat_building_lgb = lgb.train(params_cat_building, dataset_train_cat_building_lgb, dataset_validate_cat_building_lgb, feval = multiclass_score, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_cat_area_lgb = lgb.train(params_cat_area, dataset_train_cat_area_lgb, dataset_validate_cat_area_lgb, feval = multiclass_score, verbose_eval = 100)#, early_stopping_rounds = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbr = lgb.LGBMRegressor(**params_lgbr)\n",
    "#lgbc = lgb.LGBMClassifier(**params_lgbc)\n",
    "#model_reg_long_lgb_fit = lgbr.fit(X = x_train, y = y_train[:, 0], **params_lgbr_long_fit, early_stopping_rounds = 100)\n",
    "#model_reg_lat_lgb_fit = lgbr.fit(X = x_train, y = y_train[:, 1], **params_lgbr_lat_fit, early_stopping_rounds = 100)\n",
    "#model_cat_floor_lgb_fit = lgbc.fit(X = x_train, y = y_train[:, 2], **params_lgbc_floor_fit, early_stopping_rounds = 100)\n",
    "#model_cat_building_lgb_fit = lgbc.fit(X = x_train, y = y_train[:, 3], **params_lgbc_building_fit, early_stopping_rounds = 100)\n",
    "#model_cat_area_lgb_fit = lgbc.fit(X = x_train, y = y_train[:, 4], **params_lgbc_area_fit, early_stopping_rounds = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_long_train = model_reg_long_lgb.predict(x_train)\n",
    "predict_long_validate = model_reg_long_lgb.predict(x_validate)\n",
    "predict_lat_train = model_reg_lat_lgb.predict(x_train)\n",
    "predict_lat_validate = model_reg_lat_lgb.predict(x_validate)\n",
    "predict_floor_train = model_cat_floor_lgb.predict(x_train)\n",
    "predict_floor_validate = model_cat_floor_lgb.predict(x_validate)\n",
    "predict_building_train = model_cat_building_lgb.predict(x_train)\n",
    "predict_building_validate = model_cat_building_lgb.predict(x_validate)\n",
    "predict_area_train = model_cat_area_lgb.predict(x_train)\n",
    "predict_area_validate = model_cat_area_lgb.predict(x_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_long_test1 = model_reg_long_lgb.predict(x_test1)\n",
    "predict_lat_test1 = model_reg_lat_lgb.predict(x_test1)\n",
    "predict_floor_test1 = model_cat_floor_lgb.predict(x_test1)\n",
    "predict_building_test1 = model_cat_building_lgb.predict(x_test1)\n",
    "predict_area_test1 = model_cat_area_lgb.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_long_test1_fit = model_reg_long_lgb_fit.predict(x_test1)\n",
    "#predict_lat_test1_fit = model_reg_lat_lgb_fit.predict(x_test1)\n",
    "#predict_floor_test1_fit = model_cat_floor_lgb_fit.predict(x_test1)\n",
    "#predict_building_test1_fit = model_cat_building_lgb_fit.predict(x_test1)\n",
    "#predict_area_test1_fit = model_cat_area_lgb_fit.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115505e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115510e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115514e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124778e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124781e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124784e+09]]\n"
     ]
    }
   ],
   "source": [
    "#print(dataset_test.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4864931.4921   4864849.3028   4864842.961439 ... 4864790.0184\n",
      " 4864769.4062   4865010.6595  ]\n",
      "[4864827.88861666 4864848.22039905 4864832.51212763 ... 4864826.98364879\n",
      " 4864768.70698926 4864827.13400571]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:, 1])\n",
    "print(predict_lat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7680.1346     -7372.8029     -7377.07826999 ... -7401.9962\n",
      " -7368.3861     -7640.9762    ]\n",
      "[-7410.30554739 -7370.12065268 -7367.42936381 ... -7410.12964627\n",
      " -7367.99123466 -7442.58528594]\n",
      "[-7511.5215     -7535.39365217 -7348.8982     ... -7408.69525072\n",
      " -7445.55787384 -7390.7612    ]\n",
      "[-7513.01743653 -7534.592519   -7349.7597729  ... -7431.72326272\n",
      " -7448.09434054 -7360.67742061]\n",
      "[0. 4. 0. ... 0. 3. 0.]\n",
      "[3 4 1 ... 1 3 3]\n",
      "[2. 2. 3. ... 2. 0. 3.]\n",
      "[2 2 3 ... 3 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:, 0])\n",
    "print(predict_long_train)\n",
    "print(y_validate[:, 0])\n",
    "print(predict_long_validate)\n",
    "print(y_train[:, 2])\n",
    "print(predict_floor_train.argmax(axis = 1))\n",
    "print(y_validate[:, 2])\n",
    "print(predict_floor_validate.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test1[:, 0])\n",
    "print(predict_long_test1)\n",
    "print(y_test1[:, 1])\n",
    "print(predict_lat_test1)\n",
    "print(y_test1[:, 2])\n",
    "print(predict_floor_test1.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_floor_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_floor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_building_train = []\n",
    "error_building_validate = []\n",
    "error_floor_train = []\n",
    "error_floor_validate = []\n",
    "error_area_train = []\n",
    "error_area_validate = []\n",
    "predict_floor_argmax_train = predict_floor_train.argmax(axis = 1)\n",
    "predict_floor_argmax_validate = predict_floor_validate.argmax(axis = 1)\n",
    "predict_building_argmax_train = predict_building_train.argmax(axis = 1)\n",
    "predict_building_argmax_validate = predict_building_validate.argmax(axis = 1)\n",
    "predict_area_argmax_train = predict_area_train.argmax(axis = 1)\n",
    "predict_area_argmax_validate = predict_area_validate.argmax(axis = 1)\n",
    "for i in range(len(predict_floor_train)):\n",
    "    if predict_floor_argmax_train[i] != y_train[i, 2]:\n",
    "        error_floor_train.append(i)\n",
    "    if predict_building_argmax_train[i] != y_train[i, 3]:\n",
    "        error_building_train.append(i)\n",
    "    if predict_area_argmax_train[i] != y_train[i, 4]:\n",
    "        error_area_train.append(i)\n",
    "for i in range(len(predict_floor_validate)):\n",
    "    if predict_floor_argmax_validate[i] != y_validate[i, 2]:\n",
    "        error_floor_validate.append(i)\n",
    "    if predict_building_argmax_validate[i] != y_validate[i, 3]:\n",
    "        error_building_validate.append(i)\n",
    "    if predict_area_argmax_validate[i] != y_validate[i, 4]:\n",
    "        error_area_validate.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_rate_floor_train 0.012666165036368197\n",
      "error_rate_floor_validate 0.0036995234512164537\n",
      "error_rate_building_train 0.0007524454477050414\n",
      "error_rate_buildilng_validate 0.0003762227238525207\n",
      "error_rate_area_train 0.021381991472284926\n",
      "error_rate_area_validate 0.02958876629889669\n"
     ]
    }
   ],
   "source": [
    "error_rate_floor_train = len(error_floor_train) / len(predict_floor_train)\n",
    "print('error_rate_floor_train', error_rate_floor_train)\n",
    "error_rate_floor_validate = len(error_floor_validate) / len(predict_floor_train)\n",
    "print('error_rate_floor_validate', error_rate_floor_validate)\n",
    "error_rate_building_train = len(error_building_train) / len(predict_building_train)\n",
    "print('error_rate_building_train', error_rate_building_train)\n",
    "error_rate_building_validate = len(error_building_validate) / len(predict_building_train)\n",
    "print('error_rate_buildilng_validate', error_rate_building_validate)\n",
    "error_rate_area_train = len(error_area_train) / len(predict_area_train)\n",
    "print('error_rate_area_train', error_rate_area_train)\n",
    "error_rate_area_validate = len(error_area_validate) / len(predict_area_validate)\n",
    "print('error_rate_area_validate', error_rate_area_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15948\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_long_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_mean_distance_train:  8.950692895133042\n",
      "error_max_distance_train:  174.74817294686335\n",
      "error_min_distance_train:  0.006624364926024075\n",
      "error_std_distance_train:  10.924766425035246\n",
      "error_var_distance_train:  119.35052144157738\n",
      "error_mean_distance_validate:  9.417689133326265\n",
      "error_max_distance_validate:  148.5944940024184\n",
      "error_min_distance_validate:  0.10363125191523935\n",
      "error_std_distance_validate:  11.20934584112626\n",
      "error_var_distance_validate:  125.64943418597456\n"
     ]
    }
   ],
   "source": [
    "error_distance_train = []\n",
    "error_distance_validate = []\n",
    "for i in range(len(predict_long_train)):\n",
    "    error_distance_train.append(euclidean_distance(predict_lat_train[i], predict_long_train[i], y_train[i, 1], y_train[i, 0]))\n",
    "for i in range(len(predict_long_validate)):\n",
    "    error_distance_validate.append(euclidean_distance(predict_lat_validate[i], predict_long_validate[i], y_validate[i, 1], y_validate[i, 0]))\n",
    "error_mean_distance_train = np.mean(np.stack(error_distance_train)).item()\n",
    "error_max_distance_train = np.max(np.stack(error_distance_train)).item()\n",
    "error_min_distance_train = np.min(np.stack(error_distance_train)).item()\n",
    "error_std_distance_train = np.std(np.stack(error_distance_train)).item()\n",
    "error_var_distance_train = np.var(np.stack(error_distance_train)).item()\n",
    "print('error_mean_distance_train: ', error_mean_distance_train)\n",
    "print('error_max_distance_train: ', error_max_distance_train)\n",
    "print('error_min_distance_train: ', error_min_distance_train)\n",
    "print('error_std_distance_train: ', error_std_distance_train)\n",
    "print('error_var_distance_train: ', error_var_distance_train)\n",
    "error_mean_distance_validate = np.mean(np.stack(error_distance_validate)).item()\n",
    "error_max_distance_validate = np.max(np.stack(error_distance_validate)).item()\n",
    "error_min_distance_validate = np.min(np.stack(error_distance_validate)).item()\n",
    "error_std_distance_validate = np.std(np.stack(error_distance_validate)).item()\n",
    "error_var_distance_validate = np.var(np.stack(error_distance_validate)).item()\n",
    "print('error_mean_distance_validate: ', error_mean_distance_validate)\n",
    "print('error_max_distance_validate: ', error_max_distance_validate)\n",
    "print('error_min_distance_validate: ', error_min_distance_validate)\n",
    "print('error_std_distance_validate: ', error_std_distance_validate)\n",
    "print('error_var_distance_validate: ', error_var_distance_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_building_test1 = []\n",
    "error_floor_test1 = []\n",
    "error_area_test1 = []\n",
    "predict_floor_argmax_test1 = predict_floor_test1.argmax(axis = 1)\n",
    "predict_building_argmax_test1 = predict_building_test1.argmax(axis = 1)\n",
    "predict_area_argmax_test1 = predict_area_test1.argmax(axis = 1)\n",
    "for i in range(len(predict_floor_test1)):\n",
    "    if predict_floor_argmax_test1[i] != y_test1[i, 2]:\n",
    "        error_floor_test1.append(i)\n",
    "    if predict_building_argmax_test1[i] != y_test1[i, 3]:\n",
    "        error_building_test1.append(i)\n",
    "    if predict_area_argmax_test1[i] != y_test1[i, 4]:\n",
    "        error_area_test1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_rate_floor_test1 0.7027027027027027\n",
      "error_rate_building_test1 0.618018018018018\n",
      "error_rate_area_test1 0.8828828828828829\n"
     ]
    }
   ],
   "source": [
    "error_rate_floor_test1 = len(error_floor_test1) / len(predict_floor_test1)\n",
    "print('error_rate_floor_test1', error_rate_floor_test1)\n",
    "error_rate_building_test1 = len(error_building_test1) / len(predict_building_test1)\n",
    "print('error_rate_building_test1', error_rate_building_test1)\n",
    "error_rate_area_test1 = len(error_area_test1) / len(predict_area_test1)\n",
    "print('error_rate_area_test1', error_rate_area_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_mean_distance_test1:  132.64861067537527\n",
      "error_max_distance_test1:  220.85087291266163\n",
      "error_min_distance_test1:  0.19723348205642666\n",
      "error_std_distance_test1:  59.841505318076\n",
      "error_var_distance_test1:  3581.0057587333185\n"
     ]
    }
   ],
   "source": [
    "error_distance_test1 = []\n",
    "for i in range(len(predict_long_test1)):\n",
    "    error_distance_test1.append(euclidean_distance(predict_lat_test1[i], predict_long_test1[i], y_test1[i, 1], y_test1[i, 0]))\n",
    "error_mean_distance_test1 = np.mean(np.stack(error_distance_test1)).item()\n",
    "error_max_distance_test1 = np.max(np.stack(error_distance_test1)).item()\n",
    "error_min_distance_test1 = np.min(np.stack(error_distance_test1)).item()\n",
    "error_std_distance_test1 = np.std(np.stack(error_distance_test1)).item()\n",
    "error_var_distance_test1 = np.var(np.stack(error_distance_test1)).item()\n",
    "print('error_mean_distance_test1: ', error_mean_distance_test1)\n",
    "print('error_max_distance_test1: ', error_max_distance_test1)\n",
    "print('error_min_distance_test1: ', error_min_distance_test1)\n",
    "print('error_std_distance_test1: ', error_std_distance_test1)\n",
    "print('error_var_distance_test1: ', error_var_distance_test1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
