{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "\n",
    "class UJIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train = True, transform = None, target_transform = None, download = False):\n",
    "        self.root = root\n",
    "        dir_path = self.root + '/UJIndoorLoc'\n",
    "        zip_path = self.root + '/uji_uil.zip'\n",
    "        dataset_training_file = dir_path + '/trainingData.csv'\n",
    "        dataset_validation_file = dir_path + '/validationData.csv'\n",
    "        # Load independent variables (WAPs values)\n",
    "        if train:\n",
    "            dataset_file = dataset_training_file\n",
    "        else:\n",
    "            dataset_file = dataset_validation_file\n",
    "        file = open(dataset_file, 'r')\n",
    "        # Load labels\n",
    "        label = file.readline()\n",
    "        label = label.split(',')\n",
    "        # Load independent variables\n",
    "        file_load = np.loadtxt(file, delimiter = ',', skiprows = 1)\n",
    "        #file_load_label = np.loadtxt(file, delimiter = ',')\n",
    "        #data = np.genfromtxt(file, dtype = float, delimiter = ',', names = True)\n",
    "        # RSSI values\n",
    "        self.x = file_load[:, 0 : 520]\n",
    "        # Load dependent variables\n",
    "        self.y = file_load[:, 520 : 524]\n",
    "        # Divide labels into x and y\n",
    "        self.x_label = label[0 : 520]\n",
    "        self.x_label = np.concatenate([self.x_label, label[524: 529]])\n",
    "        self.y_label = label[520 : 524]\n",
    "        # Regularization of independent variables\n",
    "        self.x[self.x == 100] = np.nan    # WAP not detected\n",
    "        self.x = self.x + 104             # Convert into positive values\n",
    "        self.x = self.x / 104             # Regularize into scale between 0 and 1\n",
    "        # Building ID, Space ID, Relative Position, User ID, Phone ID and Timestamp respectively\n",
    "        self.x = np.concatenate([self.x, file_load[:, 524 : 529]], axis = 1)\n",
    "        file.close()\n",
    "        # Reduce the number of dependent variables by combining building number and floor into one variable: area\n",
    "        area = self.y[:, 3] * 5 + self.y[:, 2]\n",
    "        self.y = np.column_stack((self.y, area))\n",
    "    def to_tensor(self):\n",
    "        self.x = torch.from_numpy(self.x).float()\n",
    "        self.y = torch.from_numpy(self.y).float()\n",
    "        self.area = torch.from_numpy(self.area).float()\n",
    "    def nan_to_zero(self):\n",
    "        self.x = np.nan_to_num(self.x)\n",
    "    # Return the target instance (row)\n",
    "    def __getitem__(self, index_row):\n",
    "        return self.x[index_row, :], self.y[index_row, :]\n",
    "    # Return the number of instances (the number of rows)\n",
    "    def __len__(self, dim = 0):\n",
    "        return int(self.x.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance (unit: meter) between two coordinates in EPSG:3857 \n",
    "def euclidean_distance(latitude_1, longitude_1, latitude_2, longitude_2):\n",
    "    return np.sqrt((latitude_1 - latitude_2)**2 + (longitude_1 - longitude_2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset with lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0., 0., 1., 1.],\n",
      "        [0., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros([2,2])\n",
    "print(a)\n",
    "b = torch.ones([2,2])\n",
    "c = torch.cat((a, b), dim = 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "dataset_train = UJIDataset('./data', train = True)\n",
    "dataset_test = UJIDataset('./data', train = False)\n",
    "#gb_train_data = lgb.Dataset("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train.nan_to_zero()\n",
    "#dataset_validate.nan_to_zero()\n",
    "#dataset_test.nan_to_zero()\n",
    "#dataset_train.to_tensor()\n",
    "#dataset_validate.to_tensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37171369e+09]\n",
      " [           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37171410e+09]\n",
      " [           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37171381e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 1.80000000e+01\n",
      "  1.00000000e+01 1.37171092e+09]\n",
      " [           nan            nan            nan ... 1.80000000e+01\n",
      "  1.00000000e+01 1.37171105e+09]\n",
      " [           nan            nan            nan ... 1.80000000e+01\n",
      "  1.00000000e+01 1.37171102e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115505e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115510e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115514e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124778e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124781e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124784e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_test.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = sklearn.model_selection.train_test_split(dataset_train.x, dataset_train.y, test_size = 0.2, random_state = 42)\n",
    "x_test1, x_test2, y_test1, y_test2 = sklearn.model_selection.train_test_split(dataset_test.x, dataset_test.y, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 1.00000000e+00\n",
      "  1.40000000e+01 1.37087874e+09]\n",
      " [           nan            nan            nan ... 6.00000000e+00\n",
      "  1.90000000e+01 1.37171987e+09]\n",
      " [           nan            nan            nan ... 1.00000000e+01\n",
      "  8.00000000e+00 1.37172087e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 9.00000000e+00\n",
      "  1.40000000e+01 1.37172008e+09]\n",
      " [           nan            nan            nan ... 2.00000000e+00\n",
      "  2.30000000e+01 1.37172121e+09]\n",
      " [           nan            nan            nan ... 1.00000000e+00\n",
      "  1.40000000e+01 1.37103670e+09]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_reg_long_lgb = lgb.Dataset(x_train, label = y_train[:, 0])\n",
    "dataset_validate_reg_long_lgb = lgb.Dataset(x_validate, label = y_validate[:, 0])\n",
    "dataset_train_reg_lat_lgb = lgb.Dataset(x_train, label = y_train[:, 1])\n",
    "dataset_validate_reg_lat_lgb = lgb.Dataset(x_validate, label = y_validate[:, 1])\n",
    "dataset_train_cat_floor_lgb = lgb.Dataset(x_train, label = y_train[:, 2])\n",
    "dataset_validate_cat_floor_lgb = lgb.Dataset(x_validate, label = y_validate[:, 2])\n",
    "dataset_train_cat_building_lgb = lgb.Dataset(x_train, label = y_train[:, 3])\n",
    "dataset_validate_cat_building_lgb = lgb.Dataset(x_validate, label = y_validate[:, 3])\n",
    "dataset_train_cat_area_lgb = lgb.Dataset(x_train, label = y_train[:, 4])\n",
    "dataset_validate_cat_area_lgb = lgb.Dataset(x_validate, label = y_validate[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_reg = {'learning_rate': 0.001,\n",
    "              'num_boost_round': 1000,\n",
    "              'max_depth': 10,\n",
    "              'boosting': 'rf', \n",
    "              'objective': 'regression',\n",
    "              'metric': 'rmse', \n",
    "              'is_training_metric': True, \n",
    "              'num_leaves': 1440,  \n",
    "              #'feature_fraction': 0.7, \n",
    "              'bagging_fraction': 0.8, \n",
    "              'bagging_freq': 5,\n",
    "              'early_stopping_round': 100,\n",
    "              #'seed':2018\n",
    "             }\n",
    "params_cat_building = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 3,\n",
    "              'metric': 'multi_logloss',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              'early_stopping_round': 100,\n",
    "             }\n",
    "params_cat_floor = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 5,\n",
    "              'metric': 'multi_logloss',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              'early_stopping_round': 100,\n",
    "             }\n",
    "params_cat_area = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 15,\n",
    "              'metric': 'multi_logloss',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              'early_stopping_round': 100,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgbr = {\n",
    "              'boosting_type': 'rf',\n",
    "              'bagging_freq': 5,\n",
    "              'bagging_fraction': 0.8,\n",
    "              'num_leaves': 1440,  \n",
    "              'learning_rate': 0.001,\n",
    "              'max_depth': -1,\n",
    "              'objective': 'regression'\n",
    "              #'seed':2018\n",
    "              }\n",
    "\n",
    "params_lgbr_long_fit = {'eval_set': [(x_validate, y_validate[:, 0])],\n",
    "                        'eval_names': ['evalset_long'],\n",
    "                        'eval_metric': ['rmse'],\n",
    "                        #'early_stoppping_rounds': [100],\n",
    "                       }\n",
    "\n",
    "params_lgbr_lat_fit = {'eval_set': [(x_validate, y_validate[:, 1])],\n",
    "                        'eval_names': ['evalset_lat'],\n",
    "                        'eval_metric': ['rmse'],\n",
    "                        #'early_stoppping_rounds': [100],\n",
    "                       }\n",
    "\n",
    "params_lgbc = {'boosting_type': 'rf',\n",
    "               'bagging_freq': 5,\n",
    "               'bagging_fraction': 0.8,\n",
    "               'num_leaves': 144,  \n",
    "               'learning_rate': 0.002,\n",
    "               'max_depth': -1,\n",
    "               'objective': 'multiclass'\n",
    "              }\n",
    "\n",
    "params_lgbc_floor_fit = {'eval_set': [(x_validate, y_validate[:, 2])],\n",
    "                         'eval_names': ['evalset_floor'],\n",
    "                         'eval_metric': ['multi_logloss'],\n",
    "                         #'early_stopping_rounds': [100],\n",
    "                        }\n",
    "\n",
    "params_lgbc_building_fit = {'eval_set': [(x_validate, y_validate[:, 3])],\n",
    "                            'eval_names': ['evalset_building'],\n",
    "                            'eval_metric': ['multi_logloss'],\n",
    "                            #'early_stoppping_rounds': [100],\n",
    "                           }\n",
    "\n",
    "\n",
    "\n",
    "params_lgbc_area_fit = {'eval_set': [(x_validate, y_validate[:, 4])],\n",
    "                        'eval_names': ['evalset_area'],\n",
    "                        'eval_metric': ['multi_logloss'],\n",
    "                        #'early_stopping_rounds': [100],\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feval(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool\n",
    "#def multiclass_score(preds, train_data):\n",
    "#    label = train_data.get_label()\n",
    "#    return 'multiclass_score', np.mean(label == preds), True\n",
    "\n",
    "#def regression_score(preds, train_data):\n",
    "#    label = train_data.get_label()\n",
    "#    return 'regression_score', np.mean(euclidean_distance(label, 0, preds, 0)), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lees/anaconda3/envs/pytorch_3.7/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For early stopping, at least one dataset and eval metric is required for evaluation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7f65da30e076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_reg_long_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_reg_long_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_reg_long_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_reg_lat_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_reg_lat_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_reg_lat_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_cat_floor_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_cat_floor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_cat_floor_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_cat_floor_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_cat_building_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_cat_building\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_cat_building_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_cat_building_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_cat_area_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_cat_area\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_cat_area_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_validate_cat_area_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, early_stopping_rounds = 100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_3.7/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    262\u001b[0m                                         \u001b[0mbegin_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                                         \u001b[0mend_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                                         evaluation_result_list=evaluation_result_list))\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_3.7/lib/python3.7/site-packages/lightgbm/callback.py\u001b[0m in \u001b[0;36m_callback\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcmp_op\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_3.7/lib/python3.7/site-packages/lightgbm/callback.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             raise ValueError('For early stopping, '\n\u001b[0m\u001b[1;32m    196\u001b[0m                              'at least one dataset and eval metric is required for evaluation')\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: For early stopping, at least one dataset and eval metric is required for evaluation"
     ]
    }
   ],
   "source": [
    "#model_reg_long_lgb = lgb.train(params_reg, dataset_train_reg_long_lgb, dataset_validate_reg_long_lgb, verbose_eval = 100)# early_stopping_rounds = 100)\n",
    "#model_reg_lat_lgb = lgb.train(params_reg, dataset_train_reg_lat_lgb, dataset_validate_reg_lat_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "#model_cat_floor_lgb = lgb.train(params_cat_floor, dataset_train_cat_floor_lgb, dataset_validate_cat_floor_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "#model_cat_building_lgb = lgb.train(params_cat_building, dataset_train_cat_building_lgb, dataset_validate_cat_building_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "#model_cat_area_lgb = lgb.train(params_cat_area, dataset_train_cat_area_lgb, dataset_validate_cat_area_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbr = lgb.LGBMRegressor(**params_lgbr)\n",
    "lgbc = lgb.LGBMClassifier(**params_lgbc)\n",
    "model_reg_long_lgb_fit = lgbr.fit(X = x_train, y = y_train[:, 0], **params_lgbr_long_fit, early_stopping_rounds = 100)\n",
    "model_reg_lat_lgb_fit = lgbr.fit(X = x_train, y = y_train[:, 1], **params_lgbr_lat_fit, early_stopping_rounds = 100)\n",
    "model_cat_floor_lgb_fit = lgbc.fit(X = x_train, y = y_train[:, 2], **params_lgbc_floor_fit, early_stopping_rounds = 100)\n",
    "model_cat_building_lgb_fit = lgbc.fit(X = x_train, y = y_train[:, 3], **params_lgbc_building_fit, early_stopping_rounds = 100)\n",
    "model_cat_area_lgb_fit = lgbc.fit(X = x_train, y = y_train[:, 4], **params_lgbc_area_fit, early_stopping_rounds = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_long_train = model_reg_long_lgb.predict(x_train)\n",
    "#predict_long_validate = model_reg_long_lgb.predict(x_validate)\n",
    "#predict_lat_train = model_reg_lat_lgb.predict(x_train)\n",
    "#predict_lat_validate = model_reg_lat_lgb.predict(x_validate)\n",
    "#predict_floor_train = model_cat_floor_lgb.predict(x_train)\n",
    "#predict_floor_validate = model_cat_floor_lgb.predict(x_validate)\n",
    "#predict_building_train = model_cat_building_lgb.predict(x_train)\n",
    "#predict_building_validate = model_cat_building_lgb.predict(x_validate)\n",
    "#predict_area_train = model_cat_area_lgb.predict(x_train)\n",
    "#predict_area_validate = model_cat_area_lgb.predict(x_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_long_test1 = model_reg_long_lgb.predict(x_test1)\n",
    "#predict_lat_test1 = model_reg_lat_lgb.predict(x_test1)\n",
    "#predict_floor_test1 = model_cat_floor_lgb.predict(x_test1)\n",
    "#predict_building_test1 = model_cat_building_lgb.predict(x_test1)\n",
    "#predict_area_test1 = model_cat_area_lgb.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_long_test1_fit = model_reg_long_lgb_fit.predict(x_test1)\n",
    "predict_lat_test1_fit = model_reg_lat_lgb_fit.predict(x_test1)\n",
    "predict_floor_test1_fit = model_cat_floor_lgb_fit.predict(x_test1)\n",
    "predict_building_test1_fit = model_cat_building_lgb_fit.predict(x_test1)\n",
    "predict_area_test1_fit = model_cat_area_lgb_fit.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115505e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115510e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38115514e+09]\n",
      " ...\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124778e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124781e+09]\n",
      " [           nan            nan            nan ... 0.00000000e+00\n",
      "  1.30000000e+01 1.38124784e+09]]\n"
     ]
    }
   ],
   "source": [
    "#print(dataset_test.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4864931.4921   4864849.3028   4864842.961439 ... 4864790.0184\n",
      " 4864769.4062   4865010.6595  ]\n",
      "[4864932.18652862 4864848.93309248 4864841.04304336 ... 4864840.30335268\n",
      " 4864769.3951732  4865004.18423284]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:, 1])\n",
    "print(predict_lat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7680.1346     -7372.8029     -7377.07826999 ... -7401.9962\n",
      " -7368.3861     -7640.9762    ]\n",
      "[-7675.98644586 -7370.27021826 -7364.01905812 ... -7401.34351258\n",
      " -7365.23751479 -7640.02241201]\n",
      "[-7511.5215     -7535.39365217 -7348.8982     ... -7408.69525072\n",
      " -7445.55787384 -7390.7612    ]\n",
      "[-7513.62171703 -7535.00433142 -7346.12423151 ... -7426.8727567\n",
      " -7426.96422075 -7364.04356366]\n",
      "[0. 4. 0. ... 0. 3. 0.]\n",
      "[0 4 0 ... 0 3 0]\n",
      "[2. 2. 3. ... 2. 0. 3.]\n",
      "[2 2 3 ... 2 0 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:, 0])\n",
    "print(predict_long_train)\n",
    "print(y_validate[:, 0])\n",
    "print(predict_long_validate)\n",
    "print(y_train[:, 2])\n",
    "print(predict_floor_train.argmax(axis = 1))\n",
    "print(y_validate[:, 2])\n",
    "print(predict_floor_validate.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test1[:, 0])\n",
    "print(predict_long_test1)\n",
    "print(y_test1[:, 1])\n",
    "print(predict_lat_test1)\n",
    "print(y_test1[:, 2])\n",
    "print(predict_floor_test1.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_floor_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_floor_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_building_train = []\n",
    "error_building_validate = []\n",
    "error_floor_train = []\n",
    "error_floor_validate = []\n",
    "error_area_train = []\n",
    "error_area_validate = []\n",
    "predict_floor_argmax_train = predict_floor_train.argmax(axis = 1)\n",
    "predict_floor_argmax_validate = predict_floor_validate.argmax(axis = 1)\n",
    "predict_building_argmax_train = predict_building_train.argmax(axis = 1)\n",
    "predict_building_argmax_validate = predict_building_validate.argmax(axis = 1)\n",
    "predict_area_argmax_train = predict_area_train.argmax(axis = 1)\n",
    "predict_area_argmax_validate = predict_area_validate.argmax(axis = 1)\n",
    "for i in range(len(predict_floor_train)):\n",
    "    if predict_floor_argmax_train[i] != y_train[i, 2]:\n",
    "        error_floor_train.append(i)\n",
    "    if predict_building_argmax_train[i] != y_train[i, 3]:\n",
    "        error_building_train.append(i)\n",
    "    if predict_area_argmax_train[i] != y_train[i, 4]:\n",
    "        error_area_train.append(i)\n",
    "for i in range(len(predict_floor_validate)):\n",
    "    if predict_floor_argmax_validate[i] != y_validate[i, 2]:\n",
    "        error_floor_validate.append(i)\n",
    "    if predict_building_argmax_validate[i] != y_validate[i, 3]:\n",
    "        error_building_validate.append(i)\n",
    "    if predict_area_argmax_validate[i] != y_validate[i, 4]:\n",
    "        error_area_validate.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_rate_floor_train 0.01310509154752947\n",
      "error_rate_floor_validate 0.004075746175068974\n",
      "error_rate_building_train 0.0007524454477050414\n",
      "error_rate_buildilng_validate 0.00043892651116127413\n",
      "error_rate_area_train 0.023200401304238776\n",
      "error_rate_area_validate 0.031093279839518557\n"
     ]
    }
   ],
   "source": [
    "error_rate_floor_train = len(error_floor_train) / len(predict_floor_train)\n",
    "print('error_rate_floor_train', error_rate_floor_train)\n",
    "error_rate_floor_validate = len(error_floor_validate) / len(predict_floor_train)\n",
    "print('error_rate_floor_validate', error_rate_floor_validate)\n",
    "error_rate_building_train = len(error_building_train) / len(predict_building_train)\n",
    "print('error_rate_building_train', error_rate_building_train)\n",
    "error_rate_building_validate = len(error_building_validate) / len(predict_building_train)\n",
    "print('error_rate_buildilng_validate', error_rate_building_validate)\n",
    "error_rate_area_train = len(error_area_train) / len(predict_area_train)\n",
    "print('error_rate_area_train', error_rate_area_train)\n",
    "error_rate_area_validate = len(error_area_validate) / len(predict_area_validate)\n",
    "print('error_rate_area_validate', error_rate_area_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15948\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_long_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_mean_distance_train:  8.931841732489868\n",
      "error_max_distance_train:  175.23642677552965\n",
      "error_min_distance_train:  0.03052850093669021\n",
      "error_std_distance_train:  10.884696354216155\n",
      "error_var_distance_train:  118.47661472348646\n",
      "error_mean_distance_validate:  9.387741998798987\n",
      "error_max_distance_validate:  150.93144848482376\n",
      "error_min_distance_validate:  0.1085996046393881\n",
      "error_std_distance_validate:  11.132779555018809\n",
      "error_var_distance_validate:  123.93878062064479\n"
     ]
    }
   ],
   "source": [
    "error_distance_train = []\n",
    "error_distance_validate = []\n",
    "for i in range(len(predict_long_train)):\n",
    "    error_distance_train.append(euclidean_distance(predict_lat_train[i], predict_long_train[i], y_train[i, 1], y_train[i, 0]))\n",
    "for i in range(len(predict_long_validate)):\n",
    "    error_distance_validate.append(euclidean_distance(predict_lat_validate[i], predict_long_validate[i], y_validate[i, 1], y_validate[i, 0]))\n",
    "error_mean_distance_train = np.mean(np.stack(error_distance_train)).item()\n",
    "error_max_distance_train = np.max(np.stack(error_distance_train)).item()\n",
    "error_min_distance_train = np.min(np.stack(error_distance_train)).item()\n",
    "error_std_distance_train = np.std(np.stack(error_distance_train)).item()\n",
    "error_var_distance_train = np.var(np.stack(error_distance_train)).item()\n",
    "print('error_mean_distance_train: ', error_mean_distance_train)\n",
    "print('error_max_distance_train: ', error_max_distance_train)\n",
    "print('error_min_distance_train: ', error_min_distance_train)\n",
    "print('error_std_distance_train: ', error_std_distance_train)\n",
    "print('error_var_distance_train: ', error_var_distance_train)\n",
    "error_mean_distance_validate = np.mean(np.stack(error_distance_validate)).item()\n",
    "error_max_distance_validate = np.max(np.stack(error_distance_validate)).item()\n",
    "error_min_distance_validate = np.min(np.stack(error_distance_validate)).item()\n",
    "error_std_distance_validate = np.std(np.stack(error_distance_validate)).item()\n",
    "error_var_distance_validate = np.var(np.stack(error_distance_validate)).item()\n",
    "print('error_mean_distance_validate: ', error_mean_distance_validate)\n",
    "print('error_max_distance_validate: ', error_max_distance_validate)\n",
    "print('error_min_distance_validate: ', error_min_distance_validate)\n",
    "print('error_std_distance_validate: ', error_std_distance_validate)\n",
    "print('error_var_distance_validate: ', error_var_distance_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_building_test1 = []\n",
    "error_floor_test1 = []\n",
    "error_area_test1 = []\n",
    "predict_floor_argmax_test1 = predict_floor_test1.argmax(axis = 1)\n",
    "predict_building_argmax_test1 = predict_building_test1.argmax(axis = 1)\n",
    "predict_area_argmax_test1 = predict_area_test1.argmax(axis = 1)\n",
    "for i in range(len(predict_floor_test1)):\n",
    "    if predict_floor_argmax_test1[i] != y_test1[i, 2]:\n",
    "        error_floor_test1.append(i)\n",
    "    if predict_building_argmax_test1[i] != y_test1[i, 3]:\n",
    "        error_building_test1.append(i)\n",
    "    if predict_area_argmax_test1[i] != y_test1[i, 4]:\n",
    "        error_area_test1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_rate_floor_test1 0.6954954954954955\n",
      "error_rate_building_test1 0.7387387387387387\n",
      "error_rate_area_test1 0.8450450450450451\n"
     ]
    }
   ],
   "source": [
    "error_rate_floor_test1 = len(error_floor_test1) / len(predict_floor_test1)\n",
    "print('error_rate_floor_test1', error_rate_floor_test1)\n",
    "error_rate_building_test1 = len(error_building_test1) / len(predict_building_test1)\n",
    "print('error_rate_building_test1', error_rate_building_test1)\n",
    "error_rate_area_test1 = len(error_area_test1) / len(predict_area_test1)\n",
    "print('error_rate_area_test1', error_rate_area_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_mean_distance_test1:  132.23746685466034\n",
      "error_max_distance_test1:  220.51688028049762\n",
      "error_min_distance_test1:  1.4316358435594734\n",
      "error_std_distance_test1:  59.66302191843922\n",
      "error_var_distance_test1:  3559.676184440159\n"
     ]
    }
   ],
   "source": [
    "error_distance_test1 = []\n",
    "for i in range(len(predict_long_test1)):\n",
    "    error_distance_test1.append(euclidean_distance(predict_lat_test1[i], predict_long_test1[i], y_test1[i, 1], y_test1[i, 0]))\n",
    "error_mean_distance_test1 = np.mean(np.stack(error_distance_test1)).item()\n",
    "error_max_distance_test1 = np.max(np.stack(error_distance_test1)).item()\n",
    "error_min_distance_test1 = np.min(np.stack(error_distance_test1)).item()\n",
    "error_std_distance_test1 = np.std(np.stack(error_distance_test1)).item()\n",
    "error_var_distance_test1 = np.var(np.stack(error_distance_test1)).item()\n",
    "print('error_mean_distance_test1: ', error_mean_distance_test1)\n",
    "print('error_max_distance_test1: ', error_max_distance_test1)\n",
    "print('error_min_distance_test1: ', error_min_distance_test1)\n",
    "print('error_std_distance_test1: ', error_std_distance_test1)\n",
    "print('error_var_distance_test1: ', error_var_distance_test1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
